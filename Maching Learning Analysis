{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2360426,"sourceType":"datasetVersion","datasetId":471473}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/dwiahmad/emergency-vehicle-siren-sound-with-mfe?scriptVersionId=289500579\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport time\nimport random\nimport psutil\nimport numpy as np\nimport librosa\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport threading\nimport gc\n\n\nfrom scipy.signal import hilbert\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, regularizers, optimizers\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T09:18:53.870286Z","iopub.execute_input":"2026-01-01T09:18:53.87078Z","iopub.status.idle":"2026-01-01T09:19:21.77626Z","shell.execute_reply.started":"2026-01-01T09:18:53.870749Z","shell.execute_reply":"2026-01-01T09:19:21.775409Z"}},"outputs":[{"name":"stderr","text":"2026-01-01 09:18:57.538202: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767259137.881109      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767259137.970722      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"}],"execution_count":2},{"cell_type":"code","source":"SEED = 42\nos.environ[\"PYTHONHASHSEED\"] = str(SEED)\nnp.random.seed(SEED)\nrandom.seed(SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T09:19:21.77777Z","iopub.execute_input":"2026-01-01T09:19:21.778322Z","iopub.status.idle":"2026-01-01T09:19:21.78645Z","shell.execute_reply.started":"2026-01-01T09:19:21.778297Z","shell.execute_reply":"2026-01-01T09:19:21.785029Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class Timer:\n    \"\"\"Simple timer utility to measure elapsed wall-clock \n    time (in seconds).\"\"\"\n    def __init__(self):\n        self.start_time = None\n        self.elapsed = 0.0\n\n    def start(self):\n        self.start_time = time.perf_counter()\n\n    def stop(self):\n        if self.start_time is not None:\n            self.elapsed += time.perf_counter() - self.start_time\n            self.start_time = None\n        return self.elapsed\n\n    def reset(self):\n        self.start_time = None\n        self.elapsed = 0.0\n\n    def get_elapsed(self):\n        return self.elapsed","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T09:20:06.82981Z","iopub.execute_input":"2026-01-01T09:20:06.830164Z","iopub.status.idle":"2026-01-01T09:20:06.837115Z","shell.execute_reply.started":"2026-01-01T09:20:06.830135Z","shell.execute_reply":"2026-01-01T09:20:06.835858Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class MemoryTracker:\n    \"\"\"\n    Peak RAM tracker berbasis Peak RSS (MB).\n    - baseline: RSS tepat saat mulai tracking\n    - peak: RSS maksimum selama tracking (sampling tiap interval)\n    - delta_peak_mb = peak - baseline  (untuk klaim 'extra memory')\n    \"\"\"\n    def __init__(self, interval=0.02):\n        self.process = psutil.Process(os.getpid())\n        self.interval = float(interval)\n        self.baseline_bytes = None\n        self.peak_bytes = None\n        self._stop = False\n        self._thread = None\n\n    def _rss_bytes(self):\n        return self.process.memory_info().rss  # bytes\n\n    def _watch(self):\n        while not self._stop:\n            cur = self._rss_bytes()\n            if cur > self.peak_bytes:\n                self.peak_bytes = cur\n            time.sleep(self.interval)\n\n    def start(self):\n        self.baseline_bytes = self._rss_bytes()\n        self.peak_bytes = self.baseline_bytes\n        self._stop = False\n        self._thread = threading.Thread(target=self._watch, daemon=True)\n        self._thread.start()\n\n    def stop(self):\n        if self._thread is not None:\n            self._stop = True\n            self._thread.join()\n            self._thread = None\n\n    @property\n    def peak_mb(self):\n        return (self.peak_bytes or 0) / (1024 ** 2)\n\n    @property\n    def baseline_mb(self):\n        return (self.baseline_bytes or 0) / (1024 ** 2)\n\n    @property\n    def delta_peak_mb(self):\n        if self.baseline_bytes is None or self.peak_bytes is None:\n            return 0.0\n        return (self.peak_bytes - self.baseline_bytes) / (1024 ** 2)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T09:20:27.465661Z","iopub.execute_input":"2026-01-01T09:20:27.465997Z","iopub.status.idle":"2026-01-01T09:20:27.476713Z","shell.execute_reply.started":"2026-01-01T09:20:27.465971Z","shell.execute_reply":"2026-01-01T09:20:27.474917Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"class FeatureScaler:\n    \"\"\"\n    Standardization (zero mean, unit variance)\n    + Min-Max normalization to [0, 1].\n    Fitted only on training data within each fold.\n    \"\"\"\n    def __init__(self):\n        self.mean_ = None\n        self.std_ = None\n        self.min_ = None\n        self.max_ = None\n\n    def fit(self, X):\n        X = np.asarray(X, dtype=np.float64)\n        self.mean_ = X.mean(axis=0)\n        self.std_ = X.std(axis=0)\n        self.std_[self.std_ == 0] = 1.0  \n        X_std = (X - self.mean_) / self.std_\n\n        self.min_ = X_std.min(axis=0)\n        self.max_ = X_std.max(axis=0)\n        same = self.max_ == self.min_\n        self.max_[same] = self.min_[same] + 1.0  \n        \n    def transform(self, X):\n        X = np.asarray(X, dtype=np.float64)\n        X_std = (X - self.mean_) / self.std_\n        X_norm = (X_std - self.min_) / (self.max_ - self.min_)\n        return X_norm\n\n    def fit_transform(self, X):\n        self.fit(X)\n        return self.transform(X)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T09:20:33.904353Z","iopub.execute_input":"2026-01-01T09:20:33.904794Z","iopub.status.idle":"2026-01-01T09:20:33.912817Z","shell.execute_reply.started":"2026-01-01T09:20:33.904767Z","shell.execute_reply":"2026-01-01T09:20:33.911552Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class FeatureExtractor:\n    \"\"\"\n    Modular feature pipeline (bisa untuk B1, B2, B3, P1, P2, dan ablation):\n\n    Grup fitur (bisa di-ON/OFF):\n    - Time-domain: mean RMS, mean ZCR\n    - Basic spectral: spectral centroid, bandwidth, roll-off\n    - Min/Max freq: min & max freq > -40 dB\n    - MFCC: 13 mean MFCC\n    - Stat-MFE: 27 mel band Ã— (9 atau 4) statistik\n    - RAW MFE: log-mel 19 mel band Ã— 44 frame = 836-D (untuk P2)\n    - Chroma: 12-dim chroma STFT (mean)\n    - Doppler/motion: mean IF, mean chirp rate, mean spectral flux\n    \"\"\"\n\n    def __init__(self,\n                 target_sr=44100,\n                 duration=3.0,\n                 frame_size=1024,\n                 hop_length=512,\n                 snr_db=15.0,\n                 use_time=True,\n                 use_basic_spectral=True,\n                 use_minmaxfreq=True,\n                 use_mfcc=True,\n                 use_mfe=False,      # Stat-MFE\n                 use_raw_mfe=False,  # RAW MFE (836-D)\n                 use_chroma=True,\n                 use_doppler=True,\n                 mfe_mode=\"full\"     # \"full\" (9 stats) atau \"selected\" (4 stats)\n                 ):\n        self.target_sr = target_sr\n        self.duration = duration\n        self.frame_size = frame_size\n        self.hop_length = hop_length\n        self.snr_db = snr_db\n\n        # flags modulasi fitur\n        self.use_time = use_time\n        self.use_basic_spectral = use_basic_spectral\n        self.use_minmaxfreq = use_minmaxfreq\n        self.use_mfcc = use_mfcc\n        self.use_mfe = use_mfe          # Stat-MFE (P1 / B3)\n        self.use_raw_mfe = use_raw_mfe  # RAW MFE (P2)\n        self.use_chroma = use_chroma\n        self.use_doppler = use_doppler\n        self.mfe_mode = mfe_mode.lower() if isinstance(mfe_mode, str) else \"full\"\n\n    # ---------- Preset helper ----------\n\n    @classmethod\n    def from_preset(cls, preset_name, **kwargs):\n        \"\"\"\n        Helper untuk bikin FeatureExtractor sesuai preset:\n        - \"B1\" / \"all\": semua fitur ON (baseline B1: All Features, TANPA MFE/RAW MFE)\n        - \"mfcc_only\": hanya MFCC (B2)\n        - \"spec_no_mfcc\": centroid, SBW, roll-off, chroma, min/max freq,\n                          + Doppler (IF, chirp, flux) tanpa MFCC, tanpa time.\n        - \"pmfe\"/\"Complete MFE\": hanya Stat-MFE (full stats 9/band)\n        - \"Proposed MFE but Selected Stats\": hanya Stat-MFE (selected stats 4/band)\n        - \"raw_mfe\" / \"p2\": RAW MFE 836-D (P2)\n        kwargs bisa override parameter default (target_sr, dll.)\n        \"\"\"\n        preset = preset_name.lower()\n\n        base = dict(\n            target_sr=44100,\n            duration=3.0,\n            frame_size=1024,\n            hop_length=512,\n            snr_db=15.0,\n            use_time=True,\n            use_basic_spectral=True,\n            use_minmaxfreq=True,\n            use_mfcc=True,\n            use_mfe=False,\n            use_raw_mfe=False,\n            use_chroma=True,\n            use_doppler=True,\n            mfe_mode=\"full\",\n        )\n\n        if preset in (\"b1\", \"b1_all\", \"all\"):\n            # B1: All features (time + all spectral + doppler), TANPA MFE/RAW MFE\n            base.update(dict(\n                use_time=True,\n                use_basic_spectral=True,\n                use_minmaxfreq=True,\n                use_mfcc=True,\n                use_mfe=False,\n                use_raw_mfe=False,\n                use_chroma=True,\n                use_doppler=True,\n                mfe_mode=\"full\",\n            ))\n\n        elif preset in (\"b2_mfcc\", \"mfcc_only\"):\n            # B2: hanya MFCC (single feature group)\n            base.update(dict(\n                use_time=False,\n                use_basic_spectral=False,\n                use_minmaxfreq=False,\n                use_mfcc=True,\n                use_mfe=False,\n                use_raw_mfe=False,\n                use_chroma=False,\n                use_doppler=False,\n                mfe_mode=\"full\",\n            ))\n\n        elif preset in (\"spec_no_mfcc\", \"spectral_no_mfcc\"):\n            # Ablasi: centroid, SBW, roll-off, chroma, min/max freq, IF, chirp, flux\n            base.update(dict(\n                use_time=False,\n                use_basic_spectral=True,\n                use_minmaxfreq=True,\n                use_mfcc=False,\n                use_mfe=False,\n                use_raw_mfe=False,\n                use_chroma=True,\n                use_doppler=True,\n                mfe_mode=\"full\",\n            ))\n\n        # ---- PMFE FULL: Complete MFE (MFE-only, 9 stats per band) ----\n        elif preset in (\n            \"pmfe\",\n            \"mfe_only\",\n            \"proposed_mfe\",\n            \"complete mfe\",\n            \"complete_mfe\",\n            \"jayakumar but mfe\",\n        ):\n            base.update(dict(\n                use_time=False,\n                use_basic_spectral=False,\n                use_minmaxfreq=False,\n                use_mfcc=False,\n                use_mfe=True,\n                use_raw_mfe=False,\n                use_chroma=False,\n                use_doppler=False,\n                mfe_mode=\"full\",\n            ))\n\n        # ---- P1: Proposed MFE but Selected Stats (MFE-only, 4 stats per band) ----\n        elif preset in (\n            \"proposed mfe but selected stats\",\n            \"pmfe_selected\",\n            \"mfe_selected\",\n        ):\n            base.update(dict(\n                use_time=False,\n                use_basic_spectral=False,\n                use_minmaxfreq=False,\n                use_mfcc=False,\n                use_mfe=True,\n                use_raw_mfe=False,\n                use_chroma=False,\n                use_doppler=False,\n                mfe_mode=\"selected\",\n            ))\n\n        # ---- P2: RAW MFE ----\n        elif preset in (\"p2_raw_mfe\", \"raw_mfe\", \"p2\"):\n            # P2: RAW MFE 836-D (log-mel, no stats, flatten)\n            base.update(dict(\n                use_time=False,\n                use_basic_spectral=False,\n                use_minmaxfreq=False,\n                use_mfcc=False,\n                use_mfe=False,\n                use_raw_mfe=True,\n                use_chroma=False,\n                use_doppler=False,\n                mfe_mode=\"full\",\n            ))\n\n        else:\n            raise ValueError(f\"Unknown feature preset: {preset_name}\")\n\n        # allow override\n        base.update(kwargs)\n        return cls(**base)\n\n    # ---------- Preprocessing ----------\n\n    def preprocess(self, y, sr):\n        # Resample if needed\n        if sr != self.target_sr:\n            y = librosa.resample(y, orig_sr=sr, target_sr=self.target_sr)\n\n        # Fixed duration\n        target_len = int(self.duration * self.target_sr)\n        if len(y) > target_len:\n            y = y[:target_len]\n        elif len(y) < target_len:\n            pad_width = target_len - len(y)\n            y = np.pad(y, (0, pad_width), mode=\"constant\")\n\n        # Normalize amplitude to [-1, 1]\n        max_amp = np.max(np.abs(y)) + 1e-12\n        y = y / max_amp\n        return y, self.target_sr\n\n    # ---------- Augmentation ----------\n\n    def augment(self, y):\n        \"\"\"\n        White noise at target SNR dengan dynamic factor 0.5â€“1.5.\n        Signal dan noise dinormalisasi sebelum dicampur.\n        \"\"\"\n        signal_power = np.mean(y ** 2)\n        noise_power = signal_power / (10.0 ** (self.snr_db / 10.0))\n\n        white_noise = np.random.normal(0.0, np.sqrt(noise_power), size=len(y))\n        dynamic_factor = np.random.uniform(0.5, 1.5, size=len(y))\n        noise = white_noise * dynamic_factor\n\n        sig_max = np.max(np.abs(y)) + 1e-12\n        noise_max = np.max(np.abs(noise)) + 1e-12\n        y_norm = y / sig_max\n        noise_norm = noise / noise_max\n\n        y_aug = y_norm + noise_norm\n        y_aug = y_aug / (np.max(np.abs(y_aug)) + 1e-12)\n        return y_aug\n\n    # ---------- Time-domain ----------\n\n    def extract_time_domain_features(self, y):\n        frames = librosa.util.frame(\n            y,\n            frame_length=self.frame_size,\n            hop_length=self.hop_length\n        ).T  # (n_frames, frame_size)\n\n        rms = np.sqrt(np.mean(frames ** 2, axis=1))\n\n        sign_changes = np.diff(np.sign(frames), axis=1) != 0\n        zcr = sign_changes.sum(axis=1) / (2.0 * frames.shape[1])\n\n        mean_rms = float(np.mean(rms))\n        mean_zcr = float(np.mean(zcr))\n        return np.array([mean_rms, mean_zcr], dtype=np.float64)\n\n    # ---------- Spectrogram helper ----------\n\n    def _compute_spectrogram(self, y):\n        \"\"\"\n        Compute STFT + derived matrices once, supaya bisa dipakai\n        oleh beberapa grup fitur tanpa menghitung ulang.\n        \"\"\"\n        stft = librosa.stft(\n            y,\n            n_fft=self.frame_size,\n            hop_length=self.hop_length,\n            window=\"hann\",\n            center=False\n        )\n        P = np.abs(stft) ** 2  # power spectrogram (n_freqs, n_frames)\n        freqs = librosa.fft_frequencies(sr=self.target_sr, n_fft=self.frame_size)\n        S_db = librosa.power_to_db(P, ref=np.max)\n        return stft, P, freqs, S_db\n\n    # ---------- Basic spectral: centroid, bandwidth, rolloff ----------\n\n    def extract_basic_spectral_features(self, P, freqs):\n        magnitude_sum = P.sum(axis=0) + 1e-12\n        centroid = (freqs[:, None] * P).sum(axis=0) / magnitude_sum\n\n        diff = freqs[:, None] - centroid[None, :]\n        variance = (diff ** 2 * P).sum(axis=0) / magnitude_sum\n        bandwidth = np.sqrt(variance)\n\n        mean_centroid = float(np.mean(centroid))\n        mean_bandwidth = float(np.mean(bandwidth))\n\n        rolloff = librosa.feature.spectral_rolloff(\n            S=P,\n            sr=self.target_sr,\n            roll_percent=0.85\n        )[0]\n        mean_rolloff = float(np.mean(rolloff))\n\n        return np.array(\n            [mean_centroid, mean_bandwidth, mean_rolloff],\n            dtype=np.float64\n        )\n\n    # ---------- Min / Max frequency > -40 dB ----------\n\n    def extract_minmaxfreq_features(self, S_db, freqs):\n        significant = S_db > -40\n        min_freqs = []\n        max_freqs = []\n        for t in range(significant.shape[1]):\n            idx = np.where(significant[:, t])[0]\n            if idx.size > 0:\n                min_freqs.append(freqs[idx[0]])\n                max_freqs.append(freqs[idx[-1]])\n            else:\n                min_freqs.append(0.0)\n                max_freqs.append(0.0)\n        mean_min_freq = float(np.mean(min_freqs))\n        mean_max_freq = float(np.mean(max_freqs))\n        return np.array([mean_min_freq, mean_max_freq], dtype=np.float64)\n\n    # ---------- MFCC ----------\n\n    def extract_mfcc_features(self, P):\n        mfcc = librosa.feature.mfcc(\n            S=librosa.power_to_db(P),\n            sr=self.target_sr,\n            n_mfcc=13\n        )\n        mean_mfcc = mfcc.mean(axis=1)  # (13,)\n        return mean_mfcc.astype(np.float64)\n\n    # ---------- Proposed Stat-MFE (Mel Filterbank Energy) ----------\n\n    def extract_mfe_features(self, P):\n        \"\"\"\n        Statistical Mel-Filterbank Energy (Stat-MFE):\n        - Hitung mel-spectrogram dari power spectrogram P\n        - Konversi ke dB (log-mel)\n        - Kalau self.mfe_mode == \"full\":\n            9 statistik/band: [mean, std, min, max, median, p10, p90, p25, p75]\n          else (selected):\n            4 statistik/band: [mean, min, max, median]\n        \"\"\"\n        n_mels = 27\n\n        mel_spec = librosa.feature.melspectrogram(\n            S=P,\n            sr=self.target_sr,\n            n_fft=self.frame_size,\n            hop_length=self.hop_length,\n            n_mels=n_mels\n        )  # (27, n_frames)\n\n        mel_db = librosa.power_to_db(mel_spec, ref=np.max)  # (27, n_frames)\n\n        features_per_band = []\n        use_full = (self.mfe_mode == \"full\")\n\n        for b in range(n_mels):\n            band_vals = mel_db[b, :]\n\n            mean_val   = float(np.mean(band_vals))\n            std_val    = float(np.std(band_vals))\n            min_val    = float(np.min(band_vals))\n            max_val    = float(np.max(band_vals))\n            median_val = float(np.median(band_vals))\n\n            if use_full:\n                p10 = float(np.percentile(band_vals, 10))\n                p90 = float(np.percentile(band_vals, 90))\n                p25 = float(np.percentile(band_vals, 25))\n                p75 = float(np.percentile(band_vals, 75))\n\n                features_per_band.append([\n                    mean_val, std_val, min_val, max_val,\n                    median_val, p10, p90, p25, p75\n                ])\n            else:\n                # Selected stats: mean, min, max, median\n                features_per_band.append([\n                    mean_val, min_val, max_val, median_val\n                ])\n\n        stat_mfe = np.asarray(features_per_band, dtype=np.float64).reshape(-1)\n        return stat_mfe\n\n    # ---------- RAW MFE (P2) ----------\n\n    def extract_raw_mfe_features(self, P):\n        \"\"\"\n        RAW Mel-Filterbank Energy untuk P2:\n        - Hitung mel-spectrogram dari power spectrogram P\n        - Konversi ke dB (log-mel)\n        - Normalisasi ke representasi fixed-size:\n          19 mel band Ã— 44 frame = 836-D (flatten)\n        \"\"\"\n        n_mels = 19\n        target_frames = 44  # 19 * 44 = 836\n\n        mel_spec = librosa.feature.melspectrogram(\n            S=P,\n            sr=self.target_sr,\n            n_fft=self.frame_size,\n            hop_length=self.hop_length,\n            n_mels=n_mels\n        )  # (19, T)\n\n        mel_db = librosa.power_to_db(mel_spec, ref=np.max)  # (19, T)\n        n_bands, T = mel_db.shape\n\n        if T > target_frames:\n            # downsample time axis dengan memilih index secara merata\n            idx = np.linspace(0, T - 1, num=target_frames).astype(int)\n            mel_db_fixed = mel_db[:, idx]\n        elif T < target_frames:\n            # pad di ujung kanan dengan nilai minimum per band\n            pad_width = target_frames - T\n            pad_vals = np.min(mel_db, axis=1, keepdims=True)\n            pad = np.repeat(pad_vals, pad_width, axis=1)\n            mel_db_fixed = np.concatenate([mel_db, pad], axis=1)\n        else:\n            mel_db_fixed = mel_db\n\n        raw_mfe = mel_db_fixed.reshape(-1).astype(np.float64)  # (836,)\n        return raw_mfe\n\n    # ---------- Chroma ----------\n\n    def extract_chroma_features(self, stft):\n        chroma = librosa.feature.chroma_stft(\n            S=np.abs(stft),\n            sr=self.target_sr,\n            n_fft=self.frame_size,\n            hop_length=self.hop_length\n        )\n        mean_chroma = chroma.mean(axis=1)  # (12,)\n        return mean_chroma.astype(np.float64)\n\n    # ---------- Doppler / motion ----------\n\n    def extract_doppler_features(self, y):\n        analytic = hilbert(y)\n        phase = np.unwrap(np.angle(analytic))\n        dt = 1.0 / self.target_sr\n        inst_freq = np.diff(phase) / (2.0 * np.pi * dt)\n        mean_if = float(np.mean(inst_freq))\n\n        chirp_rate = np.diff(inst_freq) / dt\n        mean_chirp = float(np.mean(chirp_rate))\n\n        stft_mag = np.abs(librosa.stft(\n            y,\n            n_fft=self.frame_size,\n            hop_length=self.hop_length,\n            window=\"hann\",\n            center=False\n        ))\n        norm = stft_mag.sum(axis=0, keepdims=True) + 1e-12\n        S_norm = stft_mag / norm\n\n        flux_values = []\n        prev = np.zeros(S_norm.shape[0])\n        for t in range(S_norm.shape[1]):\n            cur = S_norm[:, t]\n            flux = np.sum(np.maximum(cur - prev, 0.0))\n            flux_values.append(flux)\n            prev = cur\n        mean_flux = float(np.mean(flux_values))\n\n        return np.array([mean_if, mean_chirp, mean_flux], dtype=np.float64)\n\n    # ---------- Public API ----------\n\n    def extract_features_from_signal(self, y, sr, apply_augmentation=False):\n        y_proc, sr_proc = self.preprocess(y, sr)\n        if apply_augmentation:\n            y_proc = self.augment(y_proc)\n\n        features = []\n\n        # Time-domain\n        if self.use_time:\n            features.append(self.extract_time_domain_features(y_proc))\n\n        # Spectral-domain (pakai STFT shared kalau perlu)\n        need_spectral = any([\n            self.use_basic_spectral,\n            self.use_minmaxfreq,\n            self.use_mfcc,\n            self.use_mfe,\n            self.use_raw_mfe,\n            self.use_chroma,\n        ])\n\n        stft = P = freqs = S_db = None\n        if need_spectral:\n            stft, P, freqs, S_db = self._compute_spectrogram(y_proc)\n\n        # Urutan: [basic spectral, min/max, MFCC, MFE, RAW MFE, chroma]\n        if self.use_basic_spectral and P is not None:\n            features.append(self.extract_basic_spectral_features(P, freqs))\n\n        if self.use_minmaxfreq and S_db is not None:\n            features.append(self.extract_minmaxfreq_features(S_db, freqs))\n\n        if self.use_mfcc and P is not None:\n            features.append(self.extract_mfcc_features(P))\n\n        if self.use_mfe and P is not None:\n            features.append(self.extract_mfe_features(P))\n\n        if self.use_raw_mfe and P is not None:\n            features.append(self.extract_raw_mfe_features(P))\n\n        if self.use_chroma and stft is not None:\n            features.append(self.extract_chroma_features(stft))\n\n        # Doppler / motion\n        if self.use_doppler:\n            features.append(self.extract_doppler_features(y_proc))\n\n        if not features:\n            raise ValueError(\"No feature groups enabled in FeatureExtractor.\")\n\n        full_feat = np.concatenate(features, axis=0)\n        return full_feat\n\n    def extract_features_from_file(self, filepath, label=None, augment_for_labels=None):\n        y, sr = librosa.load(filepath, sr=None, mono=True)\n\n        feats = []\n        # Original\n        feats.append(self.extract_features_from_signal(y, sr, apply_augmentation=False))\n\n        # Augmented (only for some labels)\n        if augment_for_labels is not None and label in augment_for_labels:\n            feats.append(self.extract_features_from_signal(y, sr, apply_augmentation=True))\n\n        return feats","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T09:20:44.411667Z","iopub.execute_input":"2026-01-01T09:20:44.411955Z","iopub.status.idle":"2026-01-01T09:20:44.715816Z","shell.execute_reply.started":"2026-01-01T09:20:44.411936Z","shell.execute_reply":"2026-01-01T09:20:44.714511Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"class AudioDataset:\n    \"\"\"\n    Folder structure:\n    base_dir/\n        ambulance/*.wav\n        firetruck/*.wav\n        traffic/*.wav\n\n    Menghasilkan X (fitur) dan y (label string),\n    dengan augmentasi untuk ambulance & firetruck (default).\n    \"\"\"\n    def __init__(self, base_dir, feature_extractor,\n                 class_names=None,\n                 augment=True,\n                 augment_for=(\"ambulance\", \"firetruck\")):\n        self.base_dir = base_dir\n        self.fe = feature_extractor\n        self.augment = augment\n        if class_names is None:\n            self.class_names = [\"ambulance\", \"firetruck\", \"traffic\"]\n        else:\n            self.class_names = class_names\n        self.augment_for = set(augment_for)\n\n    def load(self):\n        X = []\n        y = []\n\n        for label in self.class_names:\n            class_dir = os.path.join(self.base_dir, label)\n            if not os.path.isdir(class_dir):\n                continue\n            for fname in sorted(os.listdir(class_dir)):\n                if not fname.lower().endswith(\".wav\"):\n                    continue\n                fpath = os.path.join(class_dir, fname)\n                features_list = self.fe.extract_features_from_file(\n                    fpath,\n                    label=label,\n                    augment_for_labels=self.augment_for if self.augment else None\n                )\n                for feat in features_list:\n                    X.append(feat)\n                    y.append(label)\n\n        X = np.asarray(X, dtype=np.float64)\n        y = np.asarray(y)\n        return X, y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T09:20:54.934984Z","iopub.execute_input":"2026-01-01T09:20:54.93532Z","iopub.status.idle":"2026-01-01T09:20:54.944773Z","shell.execute_reply.started":"2026-01-01T09:20:54.935295Z","shell.execute_reply":"2026-01-01T09:20:54.943538Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def create_base_models():\n    \"\"\"Base classifiers untuk B1/B2/B3/P1/P2.\"\"\"\n    models = {\n        \"SVM\": SVC(\n            kernel=\"rbf\",\n            C=10.0,\n            gamma=\"scale\",\n            probability=True,\n            random_state=SEED\n        ),\n        \"KNN\": KNeighborsClassifier(\n            n_neighbors=3,\n            weights=\"distance\"\n        ),\n        \"RandomForest\": RandomForestClassifier(\n            n_estimators=200,\n            max_depth=None,\n            min_samples_split=2,\n            random_state=SEED,\n            n_jobs=-1\n        ),\n        \"AdaBoost\": AdaBoostClassifier(\n            n_estimators=100,\n            learning_rate=1.0,\n            random_state=SEED\n        )\n    }\n    return models","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T09:21:04.184361Z","iopub.execute_input":"2026-01-01T09:21:04.185187Z","iopub.status.idle":"2026-01-01T09:21:04.191437Z","shell.execute_reply.started":"2026-01-01T09:21:04.185147Z","shell.execute_reply":"2026-01-01T09:21:04.190305Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"class StackedEnsemble:\n    \"\"\"\n    Stacked ensemble:\n    - Base: SVM, KNN, RF, AdaBoost (probability outputs)\n    - Meta-learner: SVM (rbf)\n    \"\"\"\n    def __init__(self, base_models, meta_learner=None):\n        self.base_models = base_models\n        if meta_learner is None:\n            self.meta_learner = SVC(\n                kernel=\"rbf\",\n                C=10.0,\n                gamma=\"scale\",\n                probability=False,\n                random_state=SEED\n            )\n        else:\n            self.meta_learner = meta_learner\n\n    def fit(self, X_train, y_train):\n        X_base, X_meta, y_base, y_meta = train_test_split(\n            X_train, y_train,\n            test_size=0.2,\n            stratify=y_train,\n            random_state=SEED\n        )\n\n        for model in self.base_models.values():\n            model.fit(X_base, y_base)\n\n        meta_features = []\n        for name, model in self.base_models.items():\n            proba = model.predict_proba(X_meta)\n            meta_features.append(proba)\n        meta_X = np.hstack(meta_features)\n\n        self.meta_learner.fit(meta_X, y_meta)\n\n    def predict(self, X):\n        meta_features = []\n        for name, model in self.base_models.items():\n            proba = model.predict_proba(X)\n            meta_features.append(proba)\n        meta_X = np.hstack(meta_features)\n        y_pred = self.meta_learner.predict(meta_X)\n        return y_pred","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T09:21:13.041195Z","iopub.execute_input":"2026-01-01T09:21:13.041622Z","iopub.status.idle":"2026-01-01T09:21:13.050689Z","shell.execute_reply.started":"2026-01-01T09:21:13.04159Z","shell.execute_reply":"2026-01-01T09:21:13.049439Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# =========================\n# Experiment runner\n# =========================\n\nclass ExperimentRunner:\n    def __init__(self, data_dir, feature_preset=\"B1\"):\n        self.data_dir = data_dir\n        self.feature_preset = feature_preset\n        self.fe = FeatureExtractor.from_preset(feature_preset)\n        self.dataset = AudioDataset(data_dir, self.fe)\n        self.label_encoder = LabelEncoder()\n\n    def run(self, n_splits=5):\n        # 1. Feature extraction timing (seluruh dataset, termasuk augment)\n        feat_timer = Timer()\n        feat_mem = MemoryTracker(interval=0.02)\n\n        feat_mem.start()\n        feat_timer.start()\n        X_raw, y_labels = self.dataset.load()\n        feat_timer.stop()\n        feat_mem.stop()\n\n        feat_mem_usage = feat_mem.delta_peak_mb  # âœ… peak delta MB\n\n\n        n_samples_total = X_raw.shape[0]\n        feat_time_total = feat_timer.get_elapsed()\n        feat_time_per_sample = feat_time_total / n_samples_total\n\n        print(f\"âœ… Dataset loaded and features extracted (preset: {self.feature_preset}).\")\n        print(f\"   Total samples (incl. augmentation): {n_samples_total}\")\n        print(f\"   Feature dimension: {X_raw.shape[1]}\")\n        print(f\"   Total feature extraction time: {feat_time_total:.3f} s\")\n        print(f\"   Avg feature extraction time per sample: {feat_time_per_sample:.6f} s\")\n        print(f\"   Extra memory during feature extraction: {feat_mem_usage:.2f} MB\")\n\n        # Encode labels (0,1,2)\n        y = self.label_encoder.fit_transform(y_labels)\n\n        # 2. Cross-validation setup\n        skf = StratifiedKFold(\n            n_splits=n_splits,\n            shuffle=True,\n            random_state=SEED\n        )\n\n        model_names = [\"SVM\", \"KNN\", \"RandomForest\", \"AdaBoost\", \"StackedEnsemble\"]\n        acc_logs = {name: [] for name in model_names}\n        f1_logs = {name: [] for name in model_names}\n        infer_time_logs = {name: [] for name in model_names}  # per-sample on test set\n        mem_logs = {name: [] for name in model_names}         # train+test peak\n\n        fold_idx = 1\n        for train_idx, test_idx in skf.split(X_raw, y):\n            print(\"\\n\" + \"=\" * 60)\n            print(f\"ðŸ” FOLD {fold_idx}/{n_splits}\")\n            print(\"=\" * 60)\n\n            X_train_raw, X_test_raw = X_raw[train_idx], X_raw[test_idx]\n            y_train, y_test = y[train_idx], y[test_idx]\n\n            # Scaling (fit only on training fold)\n            scaler = FeatureScaler()\n            X_train = scaler.fit_transform(X_train_raw)\n            X_test = scaler.transform(X_test_raw)\n\n            # --- Base models ---\n            base_models = create_base_models()\n\n            for name, model in base_models.items():\n                print(f\"\\nâ–¶ Training {name}...\")\n\n                mem_tracker = MemoryTracker(interval=0.02)\n                train_timer = Timer()\n                infer_timer = Timer()\n\n                mem_tracker.start()\n\n                # Train\n                train_timer.start()\n                model.fit(X_train, y_train)\n                train_timer.stop()\n\n                # Inference on test set\n                infer_timer.start()\n                y_pred = model.predict(X_test)\n                infer_timer.stop()\n\n                mem_tracker.stop()\n\n                \n                print(f\"   {name} baseline RSS: {mem_tracker.baseline_mb:.2f} MB\")\n                print(f\"   {name} peak RSS: {mem_tracker.peak_mb:.2f} MB\")\n\n\n                mem_usage = mem_tracker.peak_mb   # âœ… PEAK RAM (RSS) selama train+test block  # âœ… peak delta MB\n\n                infer_time_total = infer_timer.get_elapsed()\n                infer_time_per_sample = infer_time_total / len(X_test)\n\n                acc = accuracy_score(y_test, y_pred)\n                f1 = f1_score(y_test, y_pred, average=\"macro\")\n\n                acc_logs[name].append(acc)\n                f1_logs[name].append(f1)\n                infer_time_logs[name].append(infer_time_per_sample)\n                mem_logs[name].append(mem_usage)\n\n                print(f\"   {name} fold accuracy: {acc:.4f}, F1-macro: {f1:.4f}\")\n                print(f\"   {name} inference time per sample (test): {infer_time_per_sample:.6f} s\")\n                print(f\"   {name} peak RAM (RSS) train+test): {mem_usage:.2f} MB\")\n                del y_pred\n                gc.collect()\n\n            # --- Stacked ensemble ---\n            print(\"\\nâ–¶ Training Stacked Ensemble (base: SVM, KNN, RF, AdaBoost; meta: SVM)...\")\n            ensemble_base_models = create_base_models()\n            ensemble = StackedEnsemble(ensemble_base_models)\n\n            mem_tracker = MemoryTracker(interval=0.02)\n            train_timer = Timer()\n            infer_timer = Timer()\n\n            mem_tracker.start()\n\n            train_timer.start()\n            ensemble.fit(X_train, y_train)\n            train_timer.stop()\n\n            infer_timer.start()\n            y_pred_ens = ensemble.predict(X_test)\n            infer_timer.stop()\n\n            mem_tracker.stop()\n            name = \"StackedEnsemble\"  \n\n            print(f\"   {name} baseline RSS: {mem_tracker.baseline_mb:.2f} MB\")\n            print(f\"   {name} peak RSS: {mem_tracker.peak_mb:.2f} MB\")\n\n\n            mem_usage = mem_tracker.peak_mb   # âœ… PEAK RAM (RSS) selama train+test block\n\n            infer_time_total = infer_timer.get_elapsed()\n            infer_time_per_sample = infer_time_total / len(X_test)\n\n            acc = accuracy_score(y_test, y_pred_ens)\n            f1 = f1_score(y_test, y_pred_ens, average=\"macro\")\n\n            name = \"StackedEnsemble\"\n            acc_logs[name].append(acc)\n            f1_logs[name].append(f1)\n            infer_time_logs[name].append(infer_time_per_sample)\n            mem_logs[name].append(mem_usage)\n\n            print(f\"   Stacked Ensemble fold accuracy: {acc:.4f}, F1-macro: {f1:.4f}\")\n            print(f\"   Stacked Ensemble inference time per sample (test): {infer_time_per_sample:.6f} s\")\n            print(f\"   Stacked Ensemble peak RAM (RSS) train+test): {mem_usage:.2f} MB\")\n\n            del y_pred_ens\n            gc.collect()\n\n            fold_idx += 1\n\n        # 3. Summarize across folds\n        summary_rows = []\n        for name in model_names:\n            acc_mean = np.mean(acc_logs[name])\n            acc_std = np.std(acc_logs[name])\n            f1_mean = np.mean(f1_logs[name])\n            f1_std = np.std(f1_logs[name])\n            infer_mean = np.mean(infer_time_logs[name])\n            mem_mean = np.mean(mem_logs[name])\n\n            summary_rows.append({\n                \"Model\": name,\n                \"Accuracy (meanÂ±std)\": f\"{acc_mean:.4f} Â± {acc_std:.4f}\",\n                \"F1 Score (meanÂ±std)\": f\"{f1_mean:.4f} Â± {f1_std:.4f}\",\n                \"Run Time Ekstraksi Fitur per Sampel [s]\": f\"{feat_time_per_sample:.6f}\",\n                \"Run Time Inference per Sampel [s] (mean test)\": f\"{infer_mean:.6f}\",\n                \"Memory usage tambahan [MB] (train+test, mean)\": f\"{mem_mean:.2f}\"\n            })\n\n        results_df = pd.DataFrame(summary_rows)\n        results_df.insert(0, \"No\", np.arange(1, len(results_df) + 1))\n\n        print(\"\\n\" + \"=\" * 80)\n        print(\"HASIL 5-FOLD CROSS VALIDATION\")\n        print(f\"(Preset fitur: {self.feature_preset})\")\n        print(\"=\" * 80)\n        print(results_df.to_string(index=False))\n\n        return results_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T09:21:22.905915Z","iopub.execute_input":"2026-01-01T09:21:22.906242Z","iopub.status.idle":"2026-01-01T09:21:22.927984Z","shell.execute_reply.started":"2026-01-01T09:21:22.906216Z","shell.execute_reply":"2026-01-01T09:21:22.926935Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    # Path yang benar berdasarkan output di atas\n    data_dir = \"/kaggle/input/emergency-vehicle-siren-sounds/sounds\"\n    \n    # Cek jumlah file audio\n    audio_files = []\n    for root, dirs, files in os.walk(data_dir):\n        audio_files.extend([f for f in files if f.endswith(('.wav', '.mp3', '.flac'))])\n    \n    print(f\"Found {len(audio_files)} audio files in {data_dir}\")\n    \n    if len(audio_files) == 0:\n        raise ValueError(\"No audio files found! Check your data_dir path.\")\n    \n    print(\"\\n=== Running Experiments ===\")\n    \n    # B1: All features (baseline Jayakumar-style, tanpa MFE/RAW MFE)\n    print(\"\\n--- Experiment B1: Baseline (All features except MFE) ---\")\n    runner_b1 = ExperimentRunner(data_dir, feature_preset=\"B1\")\n    results_b1 = runner_b1.run(n_splits=5)\n    \n    # B2: MFCC-only\n    print(\"\\n--- Experiment B2: MFCC-only ---\")\n    runner_b2 = ExperimentRunner(data_dir, feature_preset=\"mfcc_only\")\n    results_b2 = runner_b2.run(n_splits=5)\n    \n    # Ablasi: centroid, SBW, roll-off, chroma, min/max freq, IF, chirp, flux\n    print(\"\\n--- Experiment Spec: Spectral features (no MFCC) ---\")\n    runner_spec = ExperimentRunner(data_dir, feature_preset=\"spec_no_mfcc\")\n    results_spec = runner_spec.run(n_splits=5)\n    \n    # B3 / P1 (PMFE FULL): Statistical MFE-only\n    print(\"\\n--- Experiment PMFE: Complete MFE (9 stats) ---\")\n    runner_pmfe = ExperimentRunner(data_dir, feature_preset=\"Complete MFE\")\n    results_pmfe = runner_pmfe.run(n_splits=5)\n    \n    # P1: Proposed MFE with selected stats\n    print(\"\\n--- Experiment P1: Proposed MFE (Selected Stats) ---\")\n    runner_p1 = ExperimentRunner(data_dir, feature_preset=\"Proposed MFE but Selected Stats\")\n    results_p1 = runner_p1.run(n_splits=5)\n    \n    # P2: RAW MFE 836-D\n    print(\"\\n--- Experiment P2: RAW MFE (836-D) ---\")\n    runner_p2 = ExperimentRunner(data_dir, feature_preset=\"raw_mfe\")\n    results_p2 = runner_p2.run(n_splits=5)\n    \n    print(\"\\n=== All Experiments Completed ===\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T09:23:46.448413Z","iopub.execute_input":"2026-01-01T09:23:46.449718Z","iopub.status.idle":"2026-01-01T09:31:50.880327Z","shell.execute_reply.started":"2026-01-01T09:23:46.449678Z","shell.execute_reply":"2026-01-01T09:31:50.879364Z"}},"outputs":[{"name":"stdout","text":"Found 600 audio files in /kaggle/input/emergency-vehicle-siren-sounds/sounds\n\n=== Running Experiments ===\n\n--- Experiment B1: Baseline (All features except MFE) ---\nâœ… Dataset loaded and features extracted (preset: B1).\n   Total samples (incl. augmentation): 1000\n   Feature dimension: 35\n   Total feature extraction time: 89.990 s\n   Avg feature extraction time per sample: 0.089990 s\n   Extra memory during feature extraction: 185.90 MB\n\n============================================================\nðŸ” FOLD 1/5\n============================================================\n\nâ–¶ Training SVM...\n   SVM baseline RSS: 984.48 MB\n   SVM peak RSS: 984.89 MB\n   SVM fold accuracy: 0.9450, F1-macro: 0.9542\n   SVM inference time per sample (test): 0.000018 s\n   SVM peak RAM (RSS) train+test): 984.89 MB\n\nâ–¶ Training KNN...\n   KNN baseline RSS: 984.95 MB\n   KNN peak RSS: 985.52 MB\n   KNN fold accuracy: 0.9150, F1-macro: 0.9289\n   KNN inference time per sample (test): 0.000245 s\n   KNN peak RAM (RSS) train+test): 985.52 MB\n\nâ–¶ Training RandomForest...\n   RandomForest baseline RSS: 986.06 MB\n   RandomForest peak RSS: 988.93 MB\n   RandomForest fold accuracy: 0.9350, F1-macro: 0.9458\n   RandomForest inference time per sample (test): 0.000441 s\n   RandomForest peak RAM (RSS) train+test): 988.93 MB\n\nâ–¶ Training AdaBoost...\n   AdaBoost baseline RSS: 988.86 MB\n   AdaBoost peak RSS: 988.86 MB\n   AdaBoost fold accuracy: 0.7750, F1-macro: 0.8098\n   AdaBoost inference time per sample (test): 0.000066 s\n   AdaBoost peak RAM (RSS) train+test): 988.86 MB\n\nâ–¶ Training Stacked Ensemble (base: SVM, KNN, RF, AdaBoost; meta: SVM)...\n   StackedEnsemble baseline RSS: 988.95 MB\n   StackedEnsemble peak RSS: 991.09 MB\n   Stacked Ensemble fold accuracy: 0.9400, F1-macro: 0.9500\n   Stacked Ensemble inference time per sample (test): 0.000470 s\n   Stacked Ensemble peak RAM (RSS) train+test): 991.09 MB\n\n============================================================\nðŸ” FOLD 2/5\n============================================================\n\nâ–¶ Training SVM...\n   SVM baseline RSS: 991.00 MB\n   SVM peak RSS: 991.01 MB\n   SVM fold accuracy: 0.9350, F1-macro: 0.9414\n   SVM inference time per sample (test): 0.000017 s\n   SVM peak RAM (RSS) train+test): 991.01 MB\n\nâ–¶ Training KNN...\n   KNN baseline RSS: 991.00 MB\n   KNN peak RSS: 991.01 MB\n   KNN fold accuracy: 0.8900, F1-macro: 0.9054\n   KNN inference time per sample (test): 0.000013 s\n   KNN peak RAM (RSS) train+test): 991.01 MB\n\nâ–¶ Training RandomForest...\n   RandomForest baseline RSS: 991.01 MB\n   RandomForest peak RSS: 992.11 MB\n   RandomForest fold accuracy: 0.9350, F1-macro: 0.9436\n   RandomForest inference time per sample (test): 0.000456 s\n   RandomForest peak RAM (RSS) train+test): 992.11 MB\n\nâ–¶ Training AdaBoost...\n   AdaBoost baseline RSS: 992.01 MB\n   AdaBoost peak RSS: 992.02 MB\n   AdaBoost fold accuracy: 0.6000, F1-macro: 0.5524\n   AdaBoost inference time per sample (test): 0.000070 s\n   AdaBoost peak RAM (RSS) train+test): 992.02 MB\n\nâ–¶ Training Stacked Ensemble (base: SVM, KNN, RF, AdaBoost; meta: SVM)...\n   StackedEnsemble baseline RSS: 992.01 MB\n   StackedEnsemble peak RSS: 992.65 MB\n   Stacked Ensemble fold accuracy: 0.9550, F1-macro: 0.9603\n   Stacked Ensemble inference time per sample (test): 0.000528 s\n   Stacked Ensemble peak RAM (RSS) train+test): 992.65 MB\n\n============================================================\nðŸ” FOLD 3/5\n============================================================\n\nâ–¶ Training SVM...\n   SVM baseline RSS: 992.55 MB\n   SVM peak RSS: 992.55 MB\n   SVM fold accuracy: 0.9450, F1-macro: 0.9542\n   SVM inference time per sample (test): 0.000017 s\n   SVM peak RAM (RSS) train+test): 992.55 MB\n\nâ–¶ Training KNN...\n   KNN baseline RSS: 992.55 MB\n   KNN peak RSS: 992.55 MB\n   KNN fold accuracy: 0.8950, F1-macro: 0.9121\n   KNN inference time per sample (test): 0.000014 s\n   KNN peak RAM (RSS) train+test): 992.55 MB\n\nâ–¶ Training RandomForest...\n   RandomForest baseline RSS: 992.61 MB\n   RandomForest peak RSS: 992.82 MB\n   RandomForest fold accuracy: 0.9500, F1-macro: 0.9583\n   RandomForest inference time per sample (test): 0.000446 s\n   RandomForest peak RAM (RSS) train+test): 992.82 MB\n\nâ–¶ Training AdaBoost...\n   AdaBoost baseline RSS: 992.74 MB\n   AdaBoost peak RSS: 992.75 MB\n   AdaBoost fold accuracy: 0.4100, F1-macro: 0.2450\n   AdaBoost inference time per sample (test): 0.000073 s\n   AdaBoost peak RAM (RSS) train+test): 992.75 MB\n\nâ–¶ Training Stacked Ensemble (base: SVM, KNN, RF, AdaBoost; meta: SVM)...\n   StackedEnsemble baseline RSS: 992.74 MB\n   StackedEnsemble peak RSS: 993.00 MB\n   Stacked Ensemble fold accuracy: 0.9500, F1-macro: 0.9583\n   Stacked Ensemble inference time per sample (test): 0.000524 s\n   Stacked Ensemble peak RAM (RSS) train+test): 993.00 MB\n\n============================================================\nðŸ” FOLD 4/5\n============================================================\n\nâ–¶ Training SVM...\n   SVM baseline RSS: 992.91 MB\n   SVM peak RSS: 992.91 MB\n   SVM fold accuracy: 0.9750, F1-macro: 0.9792\n   SVM inference time per sample (test): 0.000017 s\n   SVM peak RAM (RSS) train+test): 992.91 MB\n\nâ–¶ Training KNN...\n   KNN baseline RSS: 992.91 MB\n   KNN peak RSS: 992.91 MB\n   KNN fold accuracy: 0.9050, F1-macro: 0.9205\n   KNN inference time per sample (test): 0.000013 s\n   KNN peak RAM (RSS) train+test): 992.91 MB\n\nâ–¶ Training RandomForest...\n   RandomForest baseline RSS: 992.91 MB\n   RandomForest peak RSS: 993.50 MB\n   RandomForest fold accuracy: 0.9500, F1-macro: 0.9583\n   RandomForest inference time per sample (test): 0.000507 s\n   RandomForest peak RAM (RSS) train+test): 993.50 MB\n\nâ–¶ Training AdaBoost...\n   AdaBoost baseline RSS: 993.39 MB\n   AdaBoost peak RSS: 993.39 MB\n   AdaBoost fold accuracy: 0.4150, F1-macro: 0.2685\n   AdaBoost inference time per sample (test): 0.000074 s\n   AdaBoost peak RAM (RSS) train+test): 993.39 MB\n\nâ–¶ Training Stacked Ensemble (base: SVM, KNN, RF, AdaBoost; meta: SVM)...\n   StackedEnsemble baseline RSS: 993.39 MB\n   StackedEnsemble peak RSS: 993.87 MB\n   Stacked Ensemble fold accuracy: 0.9700, F1-macro: 0.9750\n   Stacked Ensemble inference time per sample (test): 0.000583 s\n   Stacked Ensemble peak RAM (RSS) train+test): 993.87 MB\n\n============================================================\nðŸ” FOLD 5/5\n============================================================\n\nâ–¶ Training SVM...\n   SVM baseline RSS: 993.78 MB\n   SVM peak RSS: 993.78 MB\n   SVM fold accuracy: 0.9500, F1-macro: 0.9583\n   SVM inference time per sample (test): 0.000016 s\n   SVM peak RAM (RSS) train+test): 993.78 MB\n\nâ–¶ Training KNN...\n   KNN baseline RSS: 993.78 MB\n   KNN peak RSS: 993.78 MB\n   KNN fold accuracy: 0.9100, F1-macro: 0.9249\n   KNN inference time per sample (test): 0.000013 s\n   KNN peak RAM (RSS) train+test): 993.78 MB\n\nâ–¶ Training RandomForest...\n   RandomForest baseline RSS: 993.78 MB\n   RandomForest peak RSS: 994.22 MB\n   RandomForest fold accuracy: 0.9300, F1-macro: 0.9417\n   RandomForest inference time per sample (test): 0.000388 s\n   RandomForest peak RAM (RSS) train+test): 994.22 MB\n\nâ–¶ Training AdaBoost...\n   AdaBoost baseline RSS: 994.11 MB\n   AdaBoost peak RSS: 994.12 MB\n   AdaBoost fold accuracy: 0.4000, F1-macro: 0.2232\n   AdaBoost inference time per sample (test): 0.000067 s\n   AdaBoost peak RAM (RSS) train+test): 994.12 MB\n\nâ–¶ Training Stacked Ensemble (base: SVM, KNN, RF, AdaBoost; meta: SVM)...\n   StackedEnsemble baseline RSS: 994.11 MB\n   StackedEnsemble peak RSS: 994.48 MB\n   Stacked Ensemble fold accuracy: 0.9400, F1-macro: 0.9499\n   Stacked Ensemble inference time per sample (test): 0.000563 s\n   Stacked Ensemble peak RAM (RSS) train+test): 994.48 MB\n\n================================================================================\nHASIL 5-FOLD CROSS VALIDATION\n(Preset fitur: B1)\n================================================================================\n No           Model Accuracy (meanÂ±std) F1 Score (meanÂ±std) Run Time Ekstraksi Fitur per Sampel [s] Run Time Inference per Sampel [s] (mean test) Memory usage tambahan [MB] (train+test, mean)\n  1             SVM     0.9500 Â± 0.0134     0.9574 Â± 0.0123                                0.089990                                      0.000017                                        991.03\n  2             KNN     0.9030 Â± 0.0093     0.9184 Â± 0.0086                                0.089990                                      0.000059                                        991.16\n  3    RandomForest     0.9400 Â± 0.0084     0.9495 Â± 0.0073                                0.089990                                      0.000448                                        992.31\n  4        AdaBoost     0.5200 Â± 0.1476     0.4198 Â± 0.2288                                0.089990                                      0.000070                                        992.23\n  5 StackedEnsemble     0.9510 Â± 0.0111     0.9587 Â± 0.0092                                0.089990                                      0.000534                                        993.02\n\n--- Experiment B2: MFCC-only ---\nâœ… Dataset loaded and features extracted (preset: mfcc_only).\n   Total samples (incl. augmentation): 1000\n   Feature dimension: 13\n   Total feature extraction time: 18.540 s\n   Avg feature extraction time per sample: 0.018540 s\n   Extra memory during feature extraction: 0.00 MB\n\n============================================================\nðŸ” FOLD 1/5\n============================================================\n\nâ–¶ Training SVM...\n   SVM baseline RSS: 995.10 MB\n   SVM peak RSS: 995.10 MB\n   SVM fold accuracy: 0.8750, F1-macro: 0.8958\n   SVM inference time per sample (test): 0.000018 s\n   SVM peak RAM (RSS) train+test): 995.10 MB\n\nâ–¶ Training KNN...\n   KNN baseline RSS: 995.10 MB\n   KNN peak RSS: 995.10 MB\n   KNN fold accuracy: 0.8650, F1-macro: 0.8875\n   KNN inference time per sample (test): 0.000015 s\n   KNN peak RAM (RSS) train+test): 995.10 MB\n\nâ–¶ Training RandomForest...\n   RandomForest baseline RSS: 995.18 MB\n   RandomForest peak RSS: 996.80 MB\n   RandomForest fold accuracy: 0.8700, F1-macro: 0.8916\n   RandomForest inference time per sample (test): 0.000449 s\n   RandomForest peak RAM (RSS) train+test): 996.80 MB\n\nâ–¶ Training AdaBoost...\n   AdaBoost baseline RSS: 996.71 MB\n   AdaBoost peak RSS: 996.71 MB\n   AdaBoost fold accuracy: 0.6600, F1-macro: 0.6771\n   AdaBoost inference time per sample (test): 0.000067 s\n   AdaBoost peak RAM (RSS) train+test): 996.71 MB\n\nâ–¶ Training Stacked Ensemble (base: SVM, KNN, RF, AdaBoost; meta: SVM)...\n   StackedEnsemble baseline RSS: 996.71 MB\n   StackedEnsemble peak RSS: 997.34 MB\n   Stacked Ensemble fold accuracy: 0.8750, F1-macro: 0.8955\n   Stacked Ensemble inference time per sample (test): 0.000560 s\n   Stacked Ensemble peak RAM (RSS) train+test): 997.34 MB\n\n============================================================\nðŸ” FOLD 2/5\n============================================================\n\nâ–¶ Training SVM...\n   SVM baseline RSS: 997.25 MB\n   SVM peak RSS: 997.25 MB\n   SVM fold accuracy: 0.8950, F1-macro: 0.9122\n   SVM inference time per sample (test): 0.000019 s\n   SVM peak RAM (RSS) train+test): 997.25 MB\n\nâ–¶ Training KNN...\n   KNN baseline RSS: 997.25 MB\n   KNN peak RSS: 997.25 MB\n   KNN fold accuracy: 0.8400, F1-macro: 0.8663\n   KNN inference time per sample (test): 0.000015 s\n   KNN peak RAM (RSS) train+test): 997.25 MB\n\nâ–¶ Training RandomForest...\n   RandomForest baseline RSS: 997.25 MB\n   RandomForest peak RSS: 997.87 MB\n   RandomForest fold accuracy: 0.9150, F1-macro: 0.9289\n   RandomForest inference time per sample (test): 0.000500 s\n   RandomForest peak RAM (RSS) train+test): 997.87 MB\n\nâ–¶ Training AdaBoost...\n   AdaBoost baseline RSS: 997.79 MB\n   AdaBoost peak RSS: 997.79 MB\n   AdaBoost fold accuracy: 0.6450, F1-macro: 0.6413\n   AdaBoost inference time per sample (test): 0.000066 s\n   AdaBoost peak RAM (RSS) train+test): 997.79 MB\n\nâ–¶ Training Stacked Ensemble (base: SVM, KNN, RF, AdaBoost; meta: SVM)...\n   StackedEnsemble baseline RSS: 997.79 MB\n   StackedEnsemble peak RSS: 997.87 MB\n   Stacked Ensemble fold accuracy: 0.8800, F1-macro: 0.8999\n   Stacked Ensemble inference time per sample (test): 0.000602 s\n   Stacked Ensemble peak RAM (RSS) train+test): 997.87 MB\n\n============================================================\nðŸ” FOLD 3/5\n============================================================\n\nâ–¶ Training SVM...\n   SVM baseline RSS: 997.79 MB\n   SVM peak RSS: 997.79 MB\n   SVM fold accuracy: 0.9050, F1-macro: 0.9208\n   SVM inference time per sample (test): 0.000018 s\n   SVM peak RAM (RSS) train+test): 997.79 MB\n\nâ–¶ Training KNN...\n   KNN baseline RSS: 997.79 MB\n   KNN peak RSS: 997.79 MB\n   KNN fold accuracy: 0.9000, F1-macro: 0.9163\n   KNN inference time per sample (test): 0.000018 s\n   KNN peak RAM (RSS) train+test): 997.79 MB\n\nâ–¶ Training RandomForest...\n   RandomForest baseline RSS: 997.79 MB\n   RandomForest peak RSS: 998.16 MB\n   RandomForest fold accuracy: 0.9000, F1-macro: 0.9167\n   RandomForest inference time per sample (test): 0.000560 s\n   RandomForest peak RAM (RSS) train+test): 998.16 MB\n\nâ–¶ Training AdaBoost...\n   AdaBoost baseline RSS: 998.08 MB\n   AdaBoost peak RSS: 998.08 MB\n   AdaBoost fold accuracy: 0.7650, F1-macro: 0.7968\n   AdaBoost inference time per sample (test): 0.000065 s\n   AdaBoost peak RAM (RSS) train+test): 998.08 MB\n\nâ–¶ Training Stacked Ensemble (base: SVM, KNN, RF, AdaBoost; meta: SVM)...\n   StackedEnsemble baseline RSS: 998.08 MB\n   StackedEnsemble peak RSS: 998.19 MB\n   Stacked Ensemble fold accuracy: 0.8800, F1-macro: 0.8999\n   Stacked Ensemble inference time per sample (test): 0.000592 s\n   Stacked Ensemble peak RAM (RSS) train+test): 998.19 MB\n\n============================================================\nðŸ” FOLD 4/5\n============================================================\n\nâ–¶ Training SVM...\n   SVM baseline RSS: 998.09 MB\n   SVM peak RSS: 998.09 MB\n   SVM fold accuracy: 0.9050, F1-macro: 0.9206\n   SVM inference time per sample (test): 0.000019 s\n   SVM peak RAM (RSS) train+test): 998.09 MB\n\nâ–¶ Training KNN...\n   KNN baseline RSS: 998.09 MB\n   KNN peak RSS: 998.09 MB\n   KNN fold accuracy: 0.8850, F1-macro: 0.9039\n   KNN inference time per sample (test): 0.000016 s\n   KNN peak RAM (RSS) train+test): 998.09 MB\n\nâ–¶ Training RandomForest...\n   RandomForest baseline RSS: 998.09 MB\n   RandomForest peak RSS: 998.57 MB\n   RandomForest fold accuracy: 0.8800, F1-macro: 0.8976\n   RandomForest inference time per sample (test): 0.000449 s\n   RandomForest peak RAM (RSS) train+test): 998.57 MB\n\nâ–¶ Training AdaBoost...\n   AdaBoost baseline RSS: 998.48 MB\n   AdaBoost peak RSS: 998.48 MB\n   AdaBoost fold accuracy: 0.6550, F1-macro: 0.6816\n   AdaBoost inference time per sample (test): 0.000068 s\n   AdaBoost peak RAM (RSS) train+test): 998.48 MB\n\nâ–¶ Training Stacked Ensemble (base: SVM, KNN, RF, AdaBoost; meta: SVM)...\n   StackedEnsemble baseline RSS: 998.48 MB\n   StackedEnsemble peak RSS: 998.60 MB\n   Stacked Ensemble fold accuracy: 0.8850, F1-macro: 0.9041\n   Stacked Ensemble inference time per sample (test): 0.000540 s\n   Stacked Ensemble peak RAM (RSS) train+test): 998.60 MB\n\n============================================================\nðŸ” FOLD 5/5\n============================================================\n\nâ–¶ Training SVM...\n   SVM baseline RSS: 998.50 MB\n   SVM peak RSS: 998.50 MB\n   SVM fold accuracy: 0.9350, F1-macro: 0.9458\n   SVM inference time per sample (test): 0.000020 s\n   SVM peak RAM (RSS) train+test): 998.50 MB\n\nâ–¶ Training KNN...\n   KNN baseline RSS: 998.50 MB\n   KNN peak RSS: 998.50 MB\n   KNN fold accuracy: 0.8900, F1-macro: 0.9083\n   KNN inference time per sample (test): 0.000014 s\n   KNN peak RAM (RSS) train+test): 998.50 MB\n\nâ–¶ Training RandomForest...\n   RandomForest baseline RSS: 998.50 MB\n   RandomForest peak RSS: 998.59 MB\n   RandomForest fold accuracy: 0.9200, F1-macro: 0.9332\n   RandomForest inference time per sample (test): 0.000434 s\n   RandomForest peak RAM (RSS) train+test): 998.59 MB\n\nâ–¶ Training AdaBoost...\n   AdaBoost baseline RSS: 998.49 MB\n   AdaBoost peak RSS: 998.50 MB\n   AdaBoost fold accuracy: 0.6400, F1-macro: 0.6821\n   AdaBoost inference time per sample (test): 0.000064 s\n   AdaBoost peak RAM (RSS) train+test): 998.50 MB\n\nâ–¶ Training Stacked Ensemble (base: SVM, KNN, RF, AdaBoost; meta: SVM)...\n   StackedEnsemble baseline RSS: 998.49 MB\n   StackedEnsemble peak RSS: 998.60 MB\n   Stacked Ensemble fold accuracy: 0.9450, F1-macro: 0.9542\n   Stacked Ensemble inference time per sample (test): 0.000549 s\n   Stacked Ensemble peak RAM (RSS) train+test): 998.60 MB\n\n================================================================================\nHASIL 5-FOLD CROSS VALIDATION\n(Preset fitur: mfcc_only)\n================================================================================\n No           Model Accuracy (meanÂ±std) F1 Score (meanÂ±std) Run Time Ekstraksi Fitur per Sampel [s] Run Time Inference per Sampel [s] (mean test) Memory usage tambahan [MB] (train+test, mean)\n  1             SVM     0.9030 Â± 0.0194     0.9190 Â± 0.0162                                0.018540                                      0.000019                                        997.35\n  2             KNN     0.8760 Â± 0.0213     0.8965 Â± 0.0178                                0.018540                                      0.000016                                        997.35\n  3    RandomForest     0.8970 Â± 0.0194     0.9136 Â± 0.0165                                0.018540                                      0.000478                                        998.00\n  4        AdaBoost     0.6730 Â± 0.0465     0.6958 Â± 0.0527                                0.018540                                      0.000066                                        997.91\n  5 StackedEnsemble     0.8930 Â± 0.0262     0.9107 Â± 0.0219                                0.018540                                      0.000569                                        998.12\n\n--- Experiment Spec: Spectral features (no MFCC) ---\nâœ… Dataset loaded and features extracted (preset: spec_no_mfcc).\n   Total samples (incl. augmentation): 1000\n   Feature dimension: 20\n   Total feature extraction time: 54.266 s\n   Avg feature extraction time per sample: 0.054266 s\n   Extra memory during feature extraction: 0.00 MB\n\n============================================================\nðŸ” FOLD 1/5\n============================================================\n\nâ–¶ Training SVM...\n   SVM baseline RSS: 998.50 MB\n   SVM peak RSS: 998.50 MB\n   SVM fold accuracy: 0.9300, F1-macro: 0.9417\n   SVM inference time per sample (test): 0.000016 s\n   SVM peak RAM (RSS) train+test): 998.50 MB\n\nâ–¶ Training KNN...\n   KNN baseline RSS: 998.50 MB\n   KNN peak RSS: 998.50 MB\n   KNN fold accuracy: 0.8850, F1-macro: 0.9037\n   KNN inference time per sample (test): 0.000014 s\n   KNN peak RAM (RSS) train+test): 998.50 MB\n\nâ–¶ Training RandomForest...\n   RandomForest baseline RSS: 998.51 MB\n   RandomForest peak RSS: 998.64 MB\n   RandomForest fold accuracy: 0.9350, F1-macro: 0.9458\n   RandomForest inference time per sample (test): 0.000440 s\n   RandomForest peak RAM (RSS) train+test): 998.64 MB\n\nâ–¶ Training AdaBoost...\n   AdaBoost baseline RSS: 998.54 MB\n   AdaBoost peak RSS: 998.54 MB\n   AdaBoost fold accuracy: 0.7750, F1-macro: 0.8098\n   AdaBoost inference time per sample (test): 0.000070 s\n   AdaBoost peak RAM (RSS) train+test): 998.54 MB\n\nâ–¶ Training Stacked Ensemble (base: SVM, KNN, RF, AdaBoost; meta: SVM)...\n   StackedEnsemble baseline RSS: 998.54 MB\n   StackedEnsemble peak RSS: 998.70 MB\n   Stacked Ensemble fold accuracy: 0.9250, F1-macro: 0.9375\n   Stacked Ensemble inference time per sample (test): 0.000411 s\n   Stacked Ensemble peak RAM (RSS) train+test): 998.70 MB\n\n============================================================\nðŸ” FOLD 2/5\n============================================================\n\nâ–¶ Training SVM...\n   SVM baseline RSS: 998.61 MB\n   SVM peak RSS: 998.62 MB\n   SVM fold accuracy: 0.9300, F1-macro: 0.9374\n   SVM inference time per sample (test): 0.000017 s\n   SVM peak RAM (RSS) train+test): 998.62 MB\n\nâ–¶ Training KNN...\n   KNN baseline RSS: 998.61 MB\n   KNN peak RSS: 998.62 MB\n   KNN fold accuracy: 0.8600, F1-macro: 0.8796\n   KNN inference time per sample (test): 0.000013 s\n   KNN peak RAM (RSS) train+test): 998.62 MB\n\nâ–¶ Training RandomForest...\n   RandomForest baseline RSS: 998.62 MB\n   RandomForest peak RSS: 998.71 MB\n   RandomForest fold accuracy: 0.9350, F1-macro: 0.9437\n   RandomForest inference time per sample (test): 0.000444 s\n   RandomForest peak RAM (RSS) train+test): 998.71 MB\n\nâ–¶ Training AdaBoost...\n   AdaBoost baseline RSS: 998.62 MB\n   AdaBoost peak RSS: 998.62 MB\n   AdaBoost fold accuracy: 0.6100, F1-macro: 0.5705\n   AdaBoost inference time per sample (test): 0.000068 s\n   AdaBoost peak RAM (RSS) train+test): 998.62 MB\n\nâ–¶ Training Stacked Ensemble (base: SVM, KNN, RF, AdaBoost; meta: SVM)...\n   StackedEnsemble baseline RSS: 998.62 MB\n   StackedEnsemble peak RSS: 999.06 MB\n   Stacked Ensemble fold accuracy: 0.9250, F1-macro: 0.9353\n   Stacked Ensemble inference time per sample (test): 0.000511 s\n   Stacked Ensemble peak RAM (RSS) train+test): 999.06 MB\n\n============================================================\nðŸ” FOLD 3/5\n============================================================\n\nâ–¶ Training SVM...\n   SVM baseline RSS: 998.96 MB\n   SVM peak RSS: 998.96 MB\n   SVM fold accuracy: 0.8750, F1-macro: 0.8957\n   SVM inference time per sample (test): 0.000016 s\n   SVM peak RAM (RSS) train+test): 998.96 MB\n\nâ–¶ Training KNN...\n   KNN baseline RSS: 998.96 MB\n   KNN peak RSS: 998.96 MB\n   KNN fold accuracy: 0.8700, F1-macro: 0.8908\n   KNN inference time per sample (test): 0.000012 s\n   KNN peak RAM (RSS) train+test): 998.96 MB\n\nâ–¶ Training RandomForest...\n   RandomForest baseline RSS: 998.96 MB\n   RandomForest peak RSS: 999.12 MB\n   RandomForest fold accuracy: 0.9200, F1-macro: 0.9333\n   RandomForest inference time per sample (test): 0.000555 s\n   RandomForest peak RAM (RSS) train+test): 999.12 MB\n\nâ–¶ Training AdaBoost...\n   AdaBoost baseline RSS: 999.03 MB\n   AdaBoost peak RSS: 999.04 MB\n   AdaBoost fold accuracy: 0.4050, F1-macro: 0.2287\n   AdaBoost inference time per sample (test): 0.000092 s\n   AdaBoost peak RAM (RSS) train+test): 999.04 MB\n\nâ–¶ Training Stacked Ensemble (base: SVM, KNN, RF, AdaBoost; meta: SVM)...\n   StackedEnsemble baseline RSS: 999.03 MB\n   StackedEnsemble peak RSS: 999.12 MB\n   Stacked Ensemble fold accuracy: 0.8900, F1-macro: 0.9083\n   Stacked Ensemble inference time per sample (test): 0.000445 s\n   Stacked Ensemble peak RAM (RSS) train+test): 999.12 MB\n\n============================================================\nðŸ” FOLD 4/5\n============================================================\n\nâ–¶ Training SVM...\n   SVM baseline RSS: 999.02 MB\n   SVM peak RSS: 999.02 MB\n   SVM fold accuracy: 0.9400, F1-macro: 0.9477\n   SVM inference time per sample (test): 0.000016 s\n   SVM peak RAM (RSS) train+test): 999.02 MB\n\nâ–¶ Training KNN...\n   KNN baseline RSS: 999.02 MB\n   KNN peak RSS: 999.02 MB\n   KNN fold accuracy: 0.8600, F1-macro: 0.8804\n   KNN inference time per sample (test): 0.000018 s\n   KNN peak RAM (RSS) train+test): 999.02 MB\n\nâ–¶ Training RandomForest...\n   RandomForest baseline RSS: 999.02 MB\n   RandomForest peak RSS: 999.12 MB\n   RandomForest fold accuracy: 0.9450, F1-macro: 0.9542\n   RandomForest inference time per sample (test): 0.000457 s\n   RandomForest peak RAM (RSS) train+test): 999.12 MB\n\nâ–¶ Training AdaBoost...\n   AdaBoost baseline RSS: 999.02 MB\n   AdaBoost peak RSS: 999.03 MB\n   AdaBoost fold accuracy: 0.4150, F1-macro: 0.2685\n   AdaBoost inference time per sample (test): 0.000067 s\n   AdaBoost peak RAM (RSS) train+test): 999.03 MB\n\nâ–¶ Training Stacked Ensemble (base: SVM, KNN, RF, AdaBoost; meta: SVM)...\n   StackedEnsemble baseline RSS: 999.02 MB\n   StackedEnsemble peak RSS: 999.13 MB\n   Stacked Ensemble fold accuracy: 0.9500, F1-macro: 0.9583\n   Stacked Ensemble inference time per sample (test): 0.000508 s\n   Stacked Ensemble peak RAM (RSS) train+test): 999.13 MB\n\n============================================================\nðŸ” FOLD 5/5\n============================================================\n\nâ–¶ Training SVM...\n   SVM baseline RSS: 999.02 MB\n   SVM peak RSS: 999.02 MB\n   SVM fold accuracy: 0.9200, F1-macro: 0.9290\n   SVM inference time per sample (test): 0.000016 s\n   SVM peak RAM (RSS) train+test): 999.02 MB\n\nâ–¶ Training KNN...\n   KNN baseline RSS: 999.02 MB\n   KNN peak RSS: 999.02 MB\n   KNN fold accuracy: 0.8700, F1-macro: 0.8846\n   KNN inference time per sample (test): 0.000011 s\n   KNN peak RAM (RSS) train+test): 999.02 MB\n\nâ–¶ Training RandomForest...\n   RandomForest baseline RSS: 999.02 MB\n   RandomForest peak RSS: 999.12 MB\n   RandomForest fold accuracy: 0.9300, F1-macro: 0.9417\n   RandomForest inference time per sample (test): 0.000449 s\n   RandomForest peak RAM (RSS) train+test): 999.12 MB\n\nâ–¶ Training AdaBoost...\n   AdaBoost baseline RSS: 999.03 MB\n   AdaBoost peak RSS: 999.04 MB\n   AdaBoost fold accuracy: 0.4000, F1-macro: 0.2232\n   AdaBoost inference time per sample (test): 0.000064 s\n   AdaBoost peak RAM (RSS) train+test): 999.04 MB\n\nâ–¶ Training Stacked Ensemble (base: SVM, KNN, RF, AdaBoost; meta: SVM)...\n   StackedEnsemble baseline RSS: 999.03 MB\n   StackedEnsemble peak RSS: 999.13 MB\n   Stacked Ensemble fold accuracy: 0.9300, F1-macro: 0.9395\n   Stacked Ensemble inference time per sample (test): 0.000521 s\n   Stacked Ensemble peak RAM (RSS) train+test): 999.13 MB\n\n================================================================================\nHASIL 5-FOLD CROSS VALIDATION\n(Preset fitur: spec_no_mfcc)\n================================================================================\n No           Model Accuracy (meanÂ±std) F1 Score (meanÂ±std) Run Time Ekstraksi Fitur per Sampel [s] Run Time Inference per Sampel [s] (mean test) Memory usage tambahan [MB] (train+test, mean)\n  1             SVM     0.9190 Â± 0.0229     0.9303 Â± 0.0183                                0.054266                                      0.000016                                        998.83\n  2             KNN     0.8690 Â± 0.0092     0.8878 Â± 0.0089                                0.054266                                      0.000014                                        998.83\n  3    RandomForest     0.9330 Â± 0.0081     0.9437 Â± 0.0067                                0.054266                                      0.000469                                        998.94\n  4        AdaBoost     0.5210 Â± 0.1495     0.4201 Â± 0.2336                                0.054266                                      0.000072                                        998.85\n  5 StackedEnsemble     0.9240 Â± 0.0193     0.9358 Â± 0.0160                                0.054266                                      0.000479                                        999.03\n\n--- Experiment PMFE: Complete MFE (9 stats) ---\nâœ… Dataset loaded and features extracted (preset: Complete MFE).\n   Total samples (incl. augmentation): 1000\n   Feature dimension: 243\n   Total feature extraction time: 32.795 s\n   Avg feature extraction time per sample: 0.032795 s\n   Extra memory during feature extraction: 0.36 MB\n\n============================================================\nðŸ” FOLD 1/5\n============================================================\n\nâ–¶ Training SVM...\n   SVM baseline RSS: 999.37 MB\n   SVM peak RSS: 999.38 MB\n   SVM fold accuracy: 0.9200, F1-macro: 0.9314\n   SVM inference time per sample (test): 0.000056 s\n   SVM peak RAM (RSS) train+test): 999.38 MB\n\nâ–¶ Training KNN...\n   KNN baseline RSS: 999.37 MB\n   KNN peak RSS: 999.38 MB\n   KNN fold accuracy: 0.9350, F1-macro: 0.9439\n   KNN inference time per sample (test): 0.000024 s\n   KNN peak RAM (RSS) train+test): 999.38 MB\n\nâ–¶ Training RandomForest...\n   RandomForest baseline RSS: 1000.79 MB\n   RandomForest peak RSS: 1000.91 MB\n   RandomForest fold accuracy: 0.9600, F1-macro: 0.9667\n   RandomForest inference time per sample (test): 0.000596 s\n   RandomForest peak RAM (RSS) train+test): 1000.91 MB\n\nâ–¶ Training AdaBoost...\n   AdaBoost baseline RSS: 1000.80 MB\n   AdaBoost peak RSS: 1000.81 MB\n   AdaBoost fold accuracy: 0.9000, F1-macro: 0.9141\n   AdaBoost inference time per sample (test): 0.000102 s\n   AdaBoost peak RAM (RSS) train+test): 1000.81 MB\n\nâ–¶ Training Stacked Ensemble (base: SVM, KNN, RF, AdaBoost; meta: SVM)...\n   StackedEnsemble baseline RSS: 1000.80 MB\n   StackedEnsemble peak RSS: 1001.30 MB\n   Stacked Ensemble fold accuracy: 0.9300, F1-macro: 0.9397\n   Stacked Ensemble inference time per sample (test): 0.000518 s\n   Stacked Ensemble peak RAM (RSS) train+test): 1001.30 MB\n\n============================================================\nðŸ” FOLD 2/5\n============================================================\n\nâ–¶ Training SVM...\n   SVM baseline RSS: 1001.16 MB\n   SVM peak RSS: 1001.17 MB\n   SVM fold accuracy: 0.9650, F1-macro: 0.9708\n   SVM inference time per sample (test): 0.000058 s\n   SVM peak RAM (RSS) train+test): 1001.17 MB\n\nâ–¶ Training KNN...\n   KNN baseline RSS: 1001.16 MB\n   KNN peak RSS: 1001.17 MB\n   KNN fold accuracy: 0.9450, F1-macro: 0.9541\n   KNN inference time per sample (test): 0.000022 s\n   KNN peak RAM (RSS) train+test): 1001.17 MB\n\nâ–¶ Training RandomForest...\n   RandomForest baseline RSS: 1001.21 MB\n   RandomForest peak RSS: 1001.33 MB\n   RandomForest fold accuracy: 0.9450, F1-macro: 0.9542\n   RandomForest inference time per sample (test): 0.000508 s\n   RandomForest peak RAM (RSS) train+test): 1001.33 MB\n\nâ–¶ Training AdaBoost...\n   AdaBoost baseline RSS: 1001.25 MB\n   AdaBoost peak RSS: 1001.25 MB\n   AdaBoost fold accuracy: 0.8350, F1-macro: 0.8578\n   AdaBoost inference time per sample (test): 0.000100 s\n   AdaBoost peak RAM (RSS) train+test): 1001.25 MB\n\nâ–¶ Training Stacked Ensemble (base: SVM, KNN, RF, AdaBoost; meta: SVM)...\n   StackedEnsemble baseline RSS: 1001.25 MB\n   StackedEnsemble peak RSS: 1001.67 MB\n   Stacked Ensemble fold accuracy: 0.9700, F1-macro: 0.9750\n   Stacked Ensemble inference time per sample (test): 0.000692 s\n   Stacked Ensemble peak RAM (RSS) train+test): 1001.67 MB\n\n============================================================\nðŸ” FOLD 3/5\n============================================================\n\nâ–¶ Training SVM...\n   SVM baseline RSS: 1001.54 MB\n   SVM peak RSS: 1001.54 MB\n   SVM fold accuracy: 0.9750, F1-macro: 0.9792\n   SVM inference time per sample (test): 0.000060 s\n   SVM peak RAM (RSS) train+test): 1001.54 MB\n\nâ–¶ Training KNN...\n   KNN baseline RSS: 1001.54 MB\n   KNN peak RSS: 1001.54 MB\n   KNN fold accuracy: 0.9700, F1-macro: 0.9750\n   KNN inference time per sample (test): 0.000021 s\n   KNN peak RAM (RSS) train+test): 1001.54 MB\n\nâ–¶ Training RandomForest...\n   RandomForest baseline RSS: 1001.57 MB\n   RandomForest peak RSS: 1001.72 MB\n   RandomForest fold accuracy: 0.9650, F1-macro: 0.9708\n   RandomForest inference time per sample (test): 0.000495 s\n   RandomForest peak RAM (RSS) train+test): 1001.72 MB\n\nâ–¶ Training AdaBoost...\n   AdaBoost baseline RSS: 1001.62 MB\n   AdaBoost peak RSS: 1001.62 MB\n   AdaBoost fold accuracy: 0.9000, F1-macro: 0.9165\n   AdaBoost inference time per sample (test): 0.000101 s\n   AdaBoost peak RAM (RSS) train+test): 1001.62 MB\n\nâ–¶ Training Stacked Ensemble (base: SVM, KNN, RF, AdaBoost; meta: SVM)...\n   StackedEnsemble baseline RSS: 1001.62 MB\n   StackedEnsemble peak RSS: 1001.70 MB\n   Stacked Ensemble fold accuracy: 0.9750, F1-macro: 0.9792\n   Stacked Ensemble inference time per sample (test): 0.000575 s\n   Stacked Ensemble peak RAM (RSS) train+test): 1001.70 MB\n\n============================================================\nðŸ” FOLD 4/5\n============================================================\n\nâ–¶ Training SVM...\n   SVM baseline RSS: 1001.55 MB\n   SVM peak RSS: 1001.55 MB\n   SVM fold accuracy: 0.9550, F1-macro: 0.9625\n   SVM inference time per sample (test): 0.000061 s\n   SVM peak RAM (RSS) train+test): 1001.55 MB\n\nâ–¶ Training KNN...\n   KNN baseline RSS: 1001.55 MB\n   KNN peak RSS: 1001.55 MB\n   KNN fold accuracy: 0.9450, F1-macro: 0.9541\n   KNN inference time per sample (test): 0.000022 s\n   KNN peak RAM (RSS) train+test): 1001.55 MB\n\nâ–¶ Training RandomForest...\n   RandomForest baseline RSS: 1001.58 MB\n   RandomForest peak RSS: 1001.86 MB\n   RandomForest fold accuracy: 0.9500, F1-macro: 0.9583\n   RandomForest inference time per sample (test): 0.000500 s\n   RandomForest peak RAM (RSS) train+test): 1001.86 MB\n\nâ–¶ Training AdaBoost...\n   AdaBoost baseline RSS: 1001.76 MB\n   AdaBoost peak RSS: 1001.76 MB\n   AdaBoost fold accuracy: 0.7850, F1-macro: 0.8163\n   AdaBoost inference time per sample (test): 0.000092 s\n   AdaBoost peak RAM (RSS) train+test): 1001.76 MB\n\nâ–¶ Training Stacked Ensemble (base: SVM, KNN, RF, AdaBoost; meta: SVM)...\n   StackedEnsemble baseline RSS: 1001.76 MB\n   StackedEnsemble peak RSS: 1001.86 MB\n   Stacked Ensemble fold accuracy: 0.9550, F1-macro: 0.9625\n   Stacked Ensemble inference time per sample (test): 0.000645 s\n   Stacked Ensemble peak RAM (RSS) train+test): 1001.86 MB\n\n============================================================\nðŸ” FOLD 5/5\n============================================================\n\nâ–¶ Training SVM...\n   SVM baseline RSS: 1001.72 MB\n   SVM peak RSS: 1001.72 MB\n   SVM fold accuracy: 0.9650, F1-macro: 0.9708\n   SVM inference time per sample (test): 0.000054 s\n   SVM peak RAM (RSS) train+test): 1001.72 MB\n\nâ–¶ Training KNN...\n   KNN baseline RSS: 1001.72 MB\n   KNN peak RSS: 1001.72 MB\n   KNN fold accuracy: 0.9600, F1-macro: 0.9646\n   KNN inference time per sample (test): 0.000021 s\n   KNN peak RAM (RSS) train+test): 1001.72 MB\n\nâ–¶ Training RandomForest...\n   RandomForest baseline RSS: 1001.76 MB\n   RandomForest peak RSS: 1001.90 MB\n   RandomForest fold accuracy: 0.9700, F1-macro: 0.9750\n   RandomForest inference time per sample (test): 0.000497 s\n   RandomForest peak RAM (RSS) train+test): 1001.90 MB\n\nâ–¶ Training AdaBoost...\n   AdaBoost baseline RSS: 1001.80 MB\n   AdaBoost peak RSS: 1001.80 MB\n   AdaBoost fold accuracy: 0.7700, F1-macro: 0.8013\n   AdaBoost inference time per sample (test): 0.000102 s\n   AdaBoost peak RAM (RSS) train+test): 1001.80 MB\n\nâ–¶ Training Stacked Ensemble (base: SVM, KNN, RF, AdaBoost; meta: SVM)...\n   StackedEnsemble baseline RSS: 1001.80 MB\n   StackedEnsemble peak RSS: 1001.89 MB\n   Stacked Ensemble fold accuracy: 0.9550, F1-macro: 0.9625\n   Stacked Ensemble inference time per sample (test): 0.000513 s\n   Stacked Ensemble peak RAM (RSS) train+test): 1001.89 MB\n\n================================================================================\nHASIL 5-FOLD CROSS VALIDATION\n(Preset fitur: Complete MFE)\n================================================================================\n No           Model Accuracy (meanÂ±std) F1 Score (meanÂ±std) Run Time Ekstraksi Fitur per Sampel [s] Run Time Inference per Sampel [s] (mean test) Memory usage tambahan [MB] (train+test, mean)\n  1             SVM     0.9560 Â± 0.0191     0.9629 Â± 0.0166                                0.032795                                      0.000058                                       1001.07\n  2             KNN     0.9510 Â± 0.0124     0.9583 Â± 0.0106                                0.032795                                      0.000022                                       1001.07\n  3    RandomForest     0.9580 Â± 0.0093     0.9650 Â± 0.0077                                0.032795                                      0.000519                                       1001.55\n  4        AdaBoost     0.8380 Â± 0.0550     0.8612 Â± 0.0479                                0.032795                                      0.000099                                       1001.45\n  5 StackedEnsemble     0.9570 Â± 0.0157     0.9638 Â± 0.0138                                0.032795                                      0.000589                                       1001.68\n\n--- Experiment P1: Proposed MFE (Selected Stats) ---\nâœ… Dataset loaded and features extracted (preset: Proposed MFE but Selected Stats).\n   Total samples (incl. augmentation): 1000\n   Feature dimension: 108\n   Total feature extraction time: 20.273 s\n   Avg feature extraction time per sample: 0.020273 s\n   Extra memory during feature extraction: 0.00 MB\n\n============================================================\nðŸ” FOLD 1/5\n============================================================\n\nâ–¶ Training SVM...\n   SVM baseline RSS: 1001.75 MB\n   SVM peak RSS: 1001.75 MB\n   SVM fold accuracy: 0.9350, F1-macro: 0.9439\n   SVM inference time per sample (test): 0.000025 s\n   SVM peak RAM (RSS) train+test): 1001.75 MB\n\nâ–¶ Training KNN...\n   KNN baseline RSS: 1001.75 MB\n   KNN peak RSS: 1001.75 MB\n   KNN fold accuracy: 0.9150, F1-macro: 0.9225\n   KNN inference time per sample (test): 0.000016 s\n   KNN peak RAM (RSS) train+test): 1001.75 MB\n\nâ–¶ Training RandomForest...\n   RandomForest baseline RSS: 1001.77 MB\n   RandomForest peak RSS: 1001.89 MB\n   RandomForest fold accuracy: 0.9550, F1-macro: 0.9583\n   RandomForest inference time per sample (test): 0.000447 s\n   RandomForest peak RAM (RSS) train+test): 1001.89 MB\n\nâ–¶ Training AdaBoost...\n   AdaBoost baseline RSS: 1001.81 MB\n   AdaBoost peak RSS: 1001.81 MB\n   AdaBoost fold accuracy: 0.7050, F1-macro: 0.7220\n   AdaBoost inference time per sample (test): 0.000078 s\n   AdaBoost peak RAM (RSS) train+test): 1001.81 MB\n\nâ–¶ Training Stacked Ensemble (base: SVM, KNN, RF, AdaBoost; meta: SVM)...\n   StackedEnsemble baseline RSS: 1001.81 MB\n   StackedEnsemble peak RSS: 1002.07 MB\n   Stacked Ensemble fold accuracy: 0.9300, F1-macro: 0.9375\n   Stacked Ensemble inference time per sample (test): 0.000474 s\n   Stacked Ensemble peak RAM (RSS) train+test): 1002.07 MB\n\n============================================================\nðŸ” FOLD 2/5\n============================================================\n\nâ–¶ Training SVM...\n   SVM baseline RSS: 1001.97 MB\n   SVM peak RSS: 1001.98 MB\n   SVM fold accuracy: 0.9750, F1-macro: 0.9792\n   SVM inference time per sample (test): 0.000026 s\n   SVM peak RAM (RSS) train+test): 1001.98 MB\n\nâ–¶ Training KNN...\n   KNN baseline RSS: 1001.97 MB\n   KNN peak RSS: 1001.98 MB\n   KNN fold accuracy: 0.9550, F1-macro: 0.9603\n   KNN inference time per sample (test): 0.000015 s\n   KNN peak RAM (RSS) train+test): 1001.98 MB\n\nâ–¶ Training RandomForest...\n   RandomForest baseline RSS: 1001.98 MB\n   RandomForest peak RSS: 1002.27 MB\n   RandomForest fold accuracy: 0.9200, F1-macro: 0.9333\n   RandomForest inference time per sample (test): 0.000503 s\n   RandomForest peak RAM (RSS) train+test): 1002.27 MB\n\nâ–¶ Training AdaBoost...\n   AdaBoost baseline RSS: 1002.17 MB\n   AdaBoost peak RSS: 1002.17 MB\n   AdaBoost fold accuracy: 0.7950, F1-macro: 0.8195\n   AdaBoost inference time per sample (test): 0.000075 s\n   AdaBoost peak RAM (RSS) train+test): 1002.17 MB\n\nâ–¶ Training Stacked Ensemble (base: SVM, KNN, RF, AdaBoost; meta: SVM)...\n   StackedEnsemble baseline RSS: 1002.17 MB\n   StackedEnsemble peak RSS: 1002.59 MB\n   Stacked Ensemble fold accuracy: 0.9400, F1-macro: 0.9500\n   Stacked Ensemble inference time per sample (test): 0.000799 s\n   Stacked Ensemble peak RAM (RSS) train+test): 1002.59 MB\n\n============================================================\nðŸ” FOLD 3/5\n============================================================\n\nâ–¶ Training SVM...\n   SVM baseline RSS: 1002.49 MB\n   SVM peak RSS: 1002.49 MB\n   SVM fold accuracy: 0.9650, F1-macro: 0.9708\n   SVM inference time per sample (test): 0.000027 s\n   SVM peak RAM (RSS) train+test): 1002.49 MB\n\nâ–¶ Training KNN...\n   KNN baseline RSS: 1002.49 MB\n   KNN peak RSS: 1002.49 MB\n   KNN fold accuracy: 0.9500, F1-macro: 0.9582\n   KNN inference time per sample (test): 0.000017 s\n   KNN peak RAM (RSS) train+test): 1002.49 MB\n\nâ–¶ Training RandomForest...\n   RandomForest baseline RSS: 1002.50 MB\n   RandomForest peak RSS: 1002.59 MB\n   RandomForest fold accuracy: 0.9600, F1-macro: 0.9667\n   RandomForest inference time per sample (test): 0.000453 s\n   RandomForest peak RAM (RSS) train+test): 1002.59 MB\n\nâ–¶ Training AdaBoost...\n   AdaBoost baseline RSS: 1002.50 MB\n   AdaBoost peak RSS: 1002.50 MB\n   AdaBoost fold accuracy: 0.8550, F1-macro: 0.8791\n   AdaBoost inference time per sample (test): 0.000086 s\n   AdaBoost peak RAM (RSS) train+test): 1002.50 MB\n\nâ–¶ Training Stacked Ensemble (base: SVM, KNN, RF, AdaBoost; meta: SVM)...\n   StackedEnsemble baseline RSS: 1002.50 MB\n   StackedEnsemble peak RSS: 1002.59 MB\n   Stacked Ensemble fold accuracy: 0.9650, F1-macro: 0.9708\n   Stacked Ensemble inference time per sample (test): 0.000476 s\n   Stacked Ensemble peak RAM (RSS) train+test): 1002.59 MB\n\n============================================================\nðŸ” FOLD 4/5\n============================================================\n\nâ–¶ Training SVM...\n   SVM baseline RSS: 1002.49 MB\n   SVM peak RSS: 1002.49 MB\n   SVM fold accuracy: 0.9650, F1-macro: 0.9708\n   SVM inference time per sample (test): 0.000026 s\n   SVM peak RAM (RSS) train+test): 1002.49 MB\n\nâ–¶ Training KNN...\n   KNN baseline RSS: 1002.49 MB\n   KNN peak RSS: 1002.49 MB\n   KNN fold accuracy: 0.9350, F1-macro: 0.9457\n   KNN inference time per sample (test): 0.000016 s\n   KNN peak RAM (RSS) train+test): 1002.49 MB\n\nâ–¶ Training RandomForest...\n   RandomForest baseline RSS: 1002.50 MB\n   RandomForest peak RSS: 1002.60 MB\n   RandomForest fold accuracy: 0.9350, F1-macro: 0.9458\n   RandomForest inference time per sample (test): 0.000499 s\n   RandomForest peak RAM (RSS) train+test): 1002.60 MB\n\nâ–¶ Training AdaBoost...\n   AdaBoost baseline RSS: 1002.49 MB\n   AdaBoost peak RSS: 1002.50 MB\n   AdaBoost fold accuracy: 0.7600, F1-macro: 0.7895\n   AdaBoost inference time per sample (test): 0.000084 s\n   AdaBoost peak RAM (RSS) train+test): 1002.50 MB\n\nâ–¶ Training Stacked Ensemble (base: SVM, KNN, RF, AdaBoost; meta: SVM)...\n   StackedEnsemble baseline RSS: 1002.49 MB\n   StackedEnsemble peak RSS: 1002.59 MB\n   Stacked Ensemble fold accuracy: 0.9550, F1-macro: 0.9625\n   Stacked Ensemble inference time per sample (test): 0.000644 s\n   Stacked Ensemble peak RAM (RSS) train+test): 1002.59 MB\n\n============================================================\nðŸ” FOLD 5/5\n============================================================\n\nâ–¶ Training SVM...\n   SVM baseline RSS: 1002.49 MB\n   SVM peak RSS: 1002.49 MB\n   SVM fold accuracy: 0.9600, F1-macro: 0.9667\n   SVM inference time per sample (test): 0.000027 s\n   SVM peak RAM (RSS) train+test): 1002.49 MB\n\nâ–¶ Training KNN...\n   KNN baseline RSS: 1002.49 MB\n   KNN peak RSS: 1002.49 MB\n   KNN fold accuracy: 0.9550, F1-macro: 0.9604\n   KNN inference time per sample (test): 0.000015 s\n   KNN peak RAM (RSS) train+test): 1002.49 MB\n\nâ–¶ Training RandomForest...\n   RandomForest baseline RSS: 1002.50 MB\n   RandomForest peak RSS: 1002.59 MB\n   RandomForest fold accuracy: 0.9600, F1-macro: 0.9667\n   RandomForest inference time per sample (test): 0.000500 s\n   RandomForest peak RAM (RSS) train+test): 1002.59 MB\n\nâ–¶ Training AdaBoost...\n   AdaBoost baseline RSS: 1002.49 MB\n   AdaBoost peak RSS: 1002.50 MB\n   AdaBoost fold accuracy: 0.7650, F1-macro: 0.7918\n   AdaBoost inference time per sample (test): 0.000077 s\n   AdaBoost peak RAM (RSS) train+test): 1002.50 MB\n\nâ–¶ Training Stacked Ensemble (base: SVM, KNN, RF, AdaBoost; meta: SVM)...\n   StackedEnsemble baseline RSS: 1002.49 MB\n   StackedEnsemble peak RSS: 1002.76 MB\n   Stacked Ensemble fold accuracy: 0.9500, F1-macro: 0.9583\n   Stacked Ensemble inference time per sample (test): 0.000710 s\n   Stacked Ensemble peak RAM (RSS) train+test): 1002.76 MB\n\n================================================================================\nHASIL 5-FOLD CROSS VALIDATION\n(Preset fitur: Proposed MFE but Selected Stats)\n================================================================================\n No           Model Accuracy (meanÂ±std) F1 Score (meanÂ±std) Run Time Ekstraksi Fitur per Sampel [s] Run Time Inference per Sampel [s] (mean test) Memory usage tambahan [MB] (train+test, mean)\n  1             SVM     0.9600 Â± 0.0134     0.9663 Â± 0.0119                                0.020273                                      0.000026                                       1002.24\n  2             KNN     0.9420 Â± 0.0154     0.9494 Â± 0.0145                                0.020273                                      0.000016                                       1002.24\n  3    RandomForest     0.9460 Â± 0.0159     0.9542 Â± 0.0129                                0.020273                                      0.000480                                       1002.39\n  4        AdaBoost     0.7760 Â± 0.0490     0.8004 Â± 0.0508                                0.020273                                      0.000080                                       1002.30\n  5 StackedEnsemble     0.9480 Â± 0.0121     0.9558 Â± 0.0114                                0.020273                                      0.000621                                       1002.52\n\n--- Experiment P2: RAW MFE (836-D) ---\nâœ… Dataset loaded and features extracted (preset: raw_mfe).\n   Total samples (incl. augmentation): 1000\n   Feature dimension: 836\n   Total feature extraction time: 17.087 s\n   Avg feature extraction time per sample: 0.017087 s\n   Extra memory during feature extraction: 0.00 MB\n\n============================================================\nðŸ” FOLD 1/5\n============================================================\n\nâ–¶ Training SVM...\n   SVM baseline RSS: 1020.09 MB\n   SVM peak RSS: 1020.09 MB\n   SVM fold accuracy: 0.9150, F1-macro: 0.9291\n   SVM inference time per sample (test): 0.000204 s\n   SVM peak RAM (RSS) train+test): 1020.09 MB\n\nâ–¶ Training KNN...\n   KNN baseline RSS: 1020.09 MB\n   KNN peak RSS: 1020.09 MB\n   KNN fold accuracy: 0.8950, F1-macro: 0.8987\n   KNN inference time per sample (test): 0.000051 s\n   KNN peak RAM (RSS) train+test): 1020.09 MB\n\nâ–¶ Training RandomForest...\n   RandomForest baseline RSS: 1020.19 MB\n   RandomForest peak RSS: 1020.31 MB\n   RandomForest fold accuracy: 0.9500, F1-macro: 0.9541\n   RandomForest inference time per sample (test): 0.000935 s\n   RandomForest peak RAM (RSS) train+test): 1020.31 MB\n\nâ–¶ Training AdaBoost...\n   AdaBoost baseline RSS: 1020.21 MB\n   AdaBoost peak RSS: 1020.22 MB\n   AdaBoost fold accuracy: 0.7250, F1-macro: 0.7564\n   AdaBoost inference time per sample (test): 0.000213 s\n   AdaBoost peak RAM (RSS) train+test): 1020.22 MB\n\nâ–¶ Training Stacked Ensemble (base: SVM, KNN, RF, AdaBoost; meta: SVM)...\n   StackedEnsemble baseline RSS: 1020.21 MB\n   StackedEnsemble peak RSS: 1020.32 MB\n   Stacked Ensemble fold accuracy: 0.9350, F1-macro: 0.9458\n   Stacked Ensemble inference time per sample (test): 0.000770 s\n   Stacked Ensemble peak RAM (RSS) train+test): 1020.32 MB\n\n============================================================\nðŸ” FOLD 2/5\n============================================================\n\nâ–¶ Training SVM...\n   SVM baseline RSS: 1035.50 MB\n   SVM peak RSS: 1035.50 MB\n   SVM fold accuracy: 0.8700, F1-macro: 0.8912\n   SVM inference time per sample (test): 0.000208 s\n   SVM peak RAM (RSS) train+test): 1035.50 MB\n\nâ–¶ Training KNN...\n   KNN baseline RSS: 1035.50 MB\n   KNN peak RSS: 1035.50 MB\n   KNN fold accuracy: 0.8800, F1-macro: 0.8866\n   KNN inference time per sample (test): 0.000047 s\n   KNN peak RAM (RSS) train+test): 1035.50 MB\n\nâ–¶ Training RandomForest...\n   RandomForest baseline RSS: 1035.52 MB\n   RandomForest peak RSS: 1035.64 MB\n   RandomForest fold accuracy: 0.9250, F1-macro: 0.9375\n   RandomForest inference time per sample (test): 0.000503 s\n   RandomForest peak RAM (RSS) train+test): 1035.64 MB\n\nâ–¶ Training AdaBoost...\n   AdaBoost baseline RSS: 1035.55 MB\n   AdaBoost peak RSS: 1035.55 MB\n   AdaBoost fold accuracy: 0.6650, F1-macro: 0.6864\n   AdaBoost inference time per sample (test): 0.000182 s\n   AdaBoost peak RAM (RSS) train+test): 1035.55 MB\n\nâ–¶ Training Stacked Ensemble (base: SVM, KNN, RF, AdaBoost; meta: SVM)...\n   StackedEnsemble baseline RSS: 1035.55 MB\n   StackedEnsemble peak RSS: 1035.62 MB\n   Stacked Ensemble fold accuracy: 0.9050, F1-macro: 0.9208\n   Stacked Ensemble inference time per sample (test): 0.000729 s\n   Stacked Ensemble peak RAM (RSS) train+test): 1035.62 MB\n\n============================================================\nðŸ” FOLD 3/5\n============================================================\n\nâ–¶ Training SVM...\n   SVM baseline RSS: 1040.59 MB\n   SVM peak RSS: 1040.59 MB\n   SVM fold accuracy: 0.9300, F1-macro: 0.9416\n   SVM inference time per sample (test): 0.000197 s\n   SVM peak RAM (RSS) train+test): 1040.59 MB\n\nâ–¶ Training KNN...\n   KNN baseline RSS: 1040.59 MB\n   KNN peak RSS: 1040.59 MB\n   KNN fold accuracy: 0.9250, F1-macro: 0.9292\n   KNN inference time per sample (test): 0.000045 s\n   KNN peak RAM (RSS) train+test): 1040.59 MB\n\nâ–¶ Training RandomForest...\n   RandomForest baseline RSS: 1040.62 MB\n   RandomForest peak RSS: 1040.75 MB\n   RandomForest fold accuracy: 0.9350, F1-macro: 0.9458\n   RandomForest inference time per sample (test): 0.000452 s\n   RandomForest peak RAM (RSS) train+test): 1040.75 MB\n\nâ–¶ Training AdaBoost...\n   AdaBoost baseline RSS: 1040.66 MB\n   AdaBoost peak RSS: 1040.66 MB\n   AdaBoost fold accuracy: 0.7700, F1-macro: 0.8026\n   AdaBoost inference time per sample (test): 0.000170 s\n   AdaBoost peak RAM (RSS) train+test): 1040.66 MB\n\nâ–¶ Training Stacked Ensemble (base: SVM, KNN, RF, AdaBoost; meta: SVM)...\n   StackedEnsemble baseline RSS: 1040.66 MB\n   StackedEnsemble peak RSS: 1040.74 MB\n   Stacked Ensemble fold accuracy: 0.9300, F1-macro: 0.9414\n   Stacked Ensemble inference time per sample (test): 0.000797 s\n   Stacked Ensemble peak RAM (RSS) train+test): 1040.74 MB\n\n============================================================\nðŸ” FOLD 4/5\n============================================================\n\nâ–¶ Training SVM...\n   SVM baseline RSS: 1023.68 MB\n   SVM peak RSS: 1023.69 MB\n   SVM fold accuracy: 0.9100, F1-macro: 0.9249\n   SVM inference time per sample (test): 0.000215 s\n   SVM peak RAM (RSS) train+test): 1023.69 MB\n\nâ–¶ Training KNN...\n   KNN baseline RSS: 1023.69 MB\n   KNN peak RSS: 1023.69 MB\n   KNN fold accuracy: 0.8850, F1-macro: 0.8937\n   KNN inference time per sample (test): 0.000049 s\n   KNN peak RAM (RSS) train+test): 1023.69 MB\n\nâ–¶ Training RandomForest...\n   RandomForest baseline RSS: 1023.71 MB\n   RandomForest peak RSS: 1023.85 MB\n   RandomForest fold accuracy: 0.9250, F1-macro: 0.9375\n   RandomForest inference time per sample (test): 0.000450 s\n   RandomForest peak RAM (RSS) train+test): 1023.85 MB\n\nâ–¶ Training AdaBoost...\n   AdaBoost baseline RSS: 1023.75 MB\n   AdaBoost peak RSS: 1023.75 MB\n   AdaBoost fold accuracy: 0.7250, F1-macro: 0.7579\n   AdaBoost inference time per sample (test): 0.000174 s\n   AdaBoost peak RAM (RSS) train+test): 1023.75 MB\n\nâ–¶ Training Stacked Ensemble (base: SVM, KNN, RF, AdaBoost; meta: SVM)...\n   StackedEnsemble baseline RSS: 1023.75 MB\n   StackedEnsemble peak RSS: 1023.85 MB\n   Stacked Ensemble fold accuracy: 0.9300, F1-macro: 0.9417\n   Stacked Ensemble inference time per sample (test): 0.000743 s\n   Stacked Ensemble peak RAM (RSS) train+test): 1023.85 MB\n\n============================================================\nðŸ” FOLD 5/5\n============================================================\n\nâ–¶ Training SVM...\n   SVM baseline RSS: 1038.88 MB\n   SVM peak RSS: 1038.89 MB\n   SVM fold accuracy: 0.9500, F1-macro: 0.9583\n   SVM inference time per sample (test): 0.000196 s\n   SVM peak RAM (RSS) train+test): 1038.89 MB\n\nâ–¶ Training KNN...\n   KNN baseline RSS: 1038.88 MB\n   KNN peak RSS: 1038.89 MB\n   KNN fold accuracy: 0.8950, F1-macro: 0.9059\n   KNN inference time per sample (test): 0.000045 s\n   KNN peak RAM (RSS) train+test): 1038.89 MB\n\nâ–¶ Training RandomForest...\n   RandomForest baseline RSS: 1038.91 MB\n   RandomForest peak RSS: 1039.04 MB\n   RandomForest fold accuracy: 0.9400, F1-macro: 0.9500\n   RandomForest inference time per sample (test): 0.000501 s\n   RandomForest peak RAM (RSS) train+test): 1039.04 MB\n\nâ–¶ Training AdaBoost...\n   AdaBoost baseline RSS: 1038.93 MB\n   AdaBoost peak RSS: 1038.93 MB\n   AdaBoost fold accuracy: 0.7200, F1-macro: 0.7569\n   AdaBoost inference time per sample (test): 0.000176 s\n   AdaBoost peak RAM (RSS) train+test): 1038.93 MB\n\nâ–¶ Training Stacked Ensemble (base: SVM, KNN, RF, AdaBoost; meta: SVM)...\n   StackedEnsemble baseline RSS: 1038.93 MB\n   StackedEnsemble peak RSS: 1039.01 MB\n   Stacked Ensemble fold accuracy: 0.9400, F1-macro: 0.9499\n   Stacked Ensemble inference time per sample (test): 0.000782 s\n   Stacked Ensemble peak RAM (RSS) train+test): 1039.01 MB\n\n================================================================================\nHASIL 5-FOLD CROSS VALIDATION\n(Preset fitur: raw_mfe)\n================================================================================\n No           Model Accuracy (meanÂ±std) F1 Score (meanÂ±std) Run Time Ekstraksi Fitur per Sampel [s] Run Time Inference per Sampel [s] (mean test) Memory usage tambahan [MB] (train+test, mean)\n  1             SVM     0.9150 Â± 0.0265     0.9290 Â± 0.0222                                0.017087                                      0.000204                                       1031.75\n  2             KNN     0.8960 Â± 0.0156     0.9028 Â± 0.0146                                0.017087                                      0.000047                                       1031.75\n  3    RandomForest     0.9350 Â± 0.0095     0.9450 Â± 0.0067                                0.017087                                      0.000568                                       1031.92\n  4        AdaBoost     0.7210 Â± 0.0334     0.7520 Â± 0.0372                                0.017087                                      0.000183                                       1031.82\n  5 StackedEnsemble     0.9280 Â± 0.0121     0.9399 Â± 0.0100                                0.017087                                      0.000764                                       1031.91\n\n=== All Experiments Completed ===\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}